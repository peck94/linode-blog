---
layout: post
title: "Introduction to Machine Learning"
date: 2017-01-26
tags: ml
---
Machine learning is all the hype these days. Companies like Facebook and Google are using it for various purposes to great effect, such as spam filtering and speech and image recognition,
to personal assistants that actually work. But what is it and how does it work? Permit me to enlighten you, curious reader.

Machine learning is, at its core, "just" statistics. A common methodology in machine learning is *supervised learning*, which resembles the relationship between a student and a teacher.
Here, the student is the computer and the teacher is the machine learning researcher. The objective may be for the student to learn to recognize a certain concept, such as birds or spam e-mails,
or to learn to predict certain events. The former is called *classification*, the latter is called *regression*. In the supervised learning setting, there is a dataset $$D$$ of examples.
Formally, these examples are nothing more than pairs $$(x_i, y_i)$$ where $$x_i$$ is the input and $$y_i$$ is the correct output. The goal is for the computer to learn to give the correct output
for each input. You may think this task is trivial; after all, the computer could simply memorise all input-output pairs. This, however, is emphatically *not* what we want the computer to do
in general, just like we don't want students to blindly memorise correct answers to standard questions without understanding the content of the question. No, the goal is for the computer
to somehow "understand" the structure of the dataset and predict the correct outputs even for unseen inputs. This is impossible if the computer merely memorises all inputs, and it is vital for
machine learning to work.

So how does one make a machine learn in a non-trivial way? The answer is to (ab)use statistics! In practice, in order to make the problem of learning some dataset $$D$$ tractable, you have to
assume the examples in the dataset can be explained using a hypothesis from some relatively small set $$H$$ of candidates. We call this the *hypothesis space*. The computer needs to find out which
of the possible hypotheses $$h \in H$$ best explains the observed input-output mappings. To do this, we must define several things:

1. Our *prior*. For every hypothesis $$h \in H$$, we must specify a probability $$\Pr(h)$$ that $$h$$ is the correct hypothesis.
This probability is called a prior because it does not take into account the observable data.

2. The *likelihood*. Given the dataset $$D$$ and a hypothesis $$h$$, we must specify how likely it is that $$h$$ explains the data.
This is a probability distribution denoted $$\Pr(D \mid h)$$.

We can then find the so-called *posterior*, the probability of a given hypothesis $$h$$ explaining all data $$D$$:
\begin{equation}
    \Pr(h \mid D) = \frac{\Pr(h)\Pr(D \mid h)}{\Pr(D)}.
\end{equation}
The problem here, of course, is that we still appear to need $$\Pr(D)$$. Unless one knows the exact distribution the examples were sampled from (which you almost never do in practice),
you simply cannot know what this probability is. However, we are only interested in the hypothesis that best explains the observed data. Clearly, $$\Pr(D)$$ is independent of $$h$$,
so we may write
\begin{equation}
    \Pr(h \mid D) \propto \Pr(h)\Pr(D \mid h).
\end{equation}
We can ignore $$\Pr(D)$$ since the hypothesis $$h$$ that maximizes $$\Pr(h \mid D)$$ will obviously also maximize $$\Pr(h)\Pr(D \mid h)$$.
Denote this hypothesis by $$h^\star$$, then formally we write
\begin{equation}
    h^\star = \arg\underset{h \in H}{\max} \Pr(h)\Pr(D \mid h).
\end{equation}
This hypothesis is called a *maximum a posteriori estimate*, or MAP estimate for short.
The likelihood will usually be pretty easy to compute for a given problem, and so is pretty uncontroversial. However, the same cannot be said for the prior. In general, how do we
find out what the odds are that some hypothesis $$h$$ explains the input data without looking at said data? This is difficult and very subjective in many ways. Hence, priors are
subject to some debate. Fortunately, we can leave the prior out entirely:
\begin{equation}
    h^\star = \arg\underset{h \in H}{\max} \Pr(D \mid h).
\end{equation}
This looks reasonable too, right? Simply choose the hypothesis that maximizes the probability of obtaining the given samples.
Such a hypothesis is called a *maximum likelihood estimate*, or ML estimate for short. The advantage is it leaves out the controversial prior;
the disadvantage is an MLE will tend to "overfit", that is, it will tend to memorise the entire dataset instead of understanding it.
MAP estimates are popular in spite of the controvery surrounding priors precisely because, when a proper prior is chosen, they can prevent overfitting and lead to models that are
much more robust than the ones usually found by an MLE.

But enough theory! Time to look at an actual example, because this is where machine learning gets exciting.
Suppose we have some dataset $$D$$ of samples where the inputs and outputs are real numbers, i.e. $$x_i, y_i \in \mathbb{R}$$ for all $$i$$.
Suppose also that we suspect (e.g. by creating a scatterplot of the data) that the data was generated by a linear process, but the samples have been slightly corrupted by noise.
Formally, we posit that the input-output relation can be described as
\begin{equation}
    y_i = ax_i + b + \varepsilon_i,
\end{equation}
where $$a$$ and $$b$$ are parameters we must find and $$\varepsilon_i$$ is Gaussian noise.
Our hypothesis space thus consists of all linear functions $$f$$ of the form $$f(x) = ax + b$$. Basically, this means a hypothesis is nothing more than a pair $$(a,b)$$ of real numbers
which determine the slope and intercept of the function. Assuming the examples in $$D$$ are independent, we find
\begin{equation}
    \Pr(D \mid (a,b)) = \prod_i \Pr(y_i \mid (a,b), x_i) = \prod_i N(y_i \mid ax_i + b, \sigma^2),
\end{equation}
where $$N(y_i \mid ax_i + b, \sigma^2)$$ is a normal distribution in $$y_i$$ with mean $$ax_i + b$$ and variance $$\sigma^2$$:
\begin{equation}
    N(y_i \mid ax_i + b, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{(y_i - ax_i - b)^2}{2\sigma^2} \right).
\end{equation}
A common trick employed in such optimization problems is to maximize the *logarithm* of the quantity we actually want to maximize instead of the quantity itself directly.
This works because the logarithm preserves maxima. It also allows us to pull this stunt:

$$\begin{aligned}
    \log \Pr((a,b))\Pr(D \mid (a,b)) &= \log \Pr((a,b)) + \log \Pr(D \mid (a,b))\\
        &= \log \Pr((a,b)) + \log\prod_i \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{(y_i - ax_i - b)^2}{2\sigma^2} \right)\\
        &= \log \Pr((a,b)) + \sum_i\log \frac{1}{\sigma\sqrt{2\pi}}\exp\left( -\frac{(y_i - ax_i - b)^2}{2\sigma^2} \right)\\
        &= \log \Pr((a,b)) + \sum_i\log \frac{1}{\sigma\sqrt{2\pi}} - \frac{(y_i - ax_i - b)^2}{2\sigma^2}\\
        &\propto \log \Pr((a,b)) - \sum_i (y_i - ax_i - b)^2.
\end{aligned}$$
In the last step, we have simply dropped all terms that are constant with respect to $$a$$ and $$b$$, since they won't affect the optimum anyway.
What's left is an expression that conveys quite a bit of useful information: in order to obtain the MAP estimate, we must maximize the logarithm of
the prior and minimize the squared error of the hypothesis. For an ML estimate, the resulting expression is the same but without the prior:

$$\begin{aligned}
    \log \Pr(D \mid (a,b)) &\propto -\sum_i (y_i - ax_i - b)^2.
\end{aligned}$$

As mentioned before, however, the prior can prove useful to prevent overfitting. For example, we might find it unlikely that the underlying linear function has large coefficients.
To express this belief, we might add in the following assumption:
\begin{equation}
    \Pr((a,b)) = N(a+b \mid 0, 1).
\end{equation}
Plugging this into the MAP estimate, this yields
$$\begin{aligned}
    \log \Pr((a,b))\Pr(D \mid (a,b)) &\propto \log \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(a+b)^2}{2}\right) - \sum_i (y_i - ax_i - b)^2\\
        &\propto -(a+b)^2 - \sum_i (y_i - ax_i - b)^2.
\end{aligned}$$
Hence, the model will be biased towards linear functions with small coefficients.

The above was an example of what is called *linear regression*, but this is a very limited application. Although linear regression is very useful in many practical applications,
it is not the reason why machine learning is as popular as it is. No, machine learning is popular because of *deep neural networks*. These are state-of-the-art statistical models
that yield very good performance (sometimes even superhuman performance!) on many tasks previously thought almost impossible for a machine to accomplish, such as image recognition.
Though it may sound intimidating, deep neural networks (in fact, neural networks in general) are very simple to describe mathematically at a high level.
Take your standard image classification task: you have a dataset $$D$$ of inputs $$x_i \in \mathbb{R}^d$$ and outputs $$y_i \in \{1, \dots, C\}$$.
That is, the inputs are vectors of real numbers and the outputs are the classes to which these inputs belong.
For example, the different classes $$1, \dots, C$$ may correspond with different animals appearing in the image, and it is the job of the classifier to determine which animal is present
in a given sample. The way this is done is very much like the above example of linear regression: we assume the output classes follow some pre-defined statistical distribution
of which we need to find the correct parameters. The simplest case is *binary classification*, when there are only two classes. Here, we assume the output classes follow a
*Bernouilli distribution*:

$$\begin{aligned}
    \Pr(y \mid \theta) &= \left\{\begin{array}{ll}
        \theta & y = 1\\
        1 - \theta & y = 0
    \end{array}\right.
\end{aligned}$$

Here, $$\theta$$ is a parameter in $$[0,1]$$ specifying the probability that a sample belongs to class 1.
Now, in a typical neural network, the assumption is that $$\theta$$ is a non-linear function of $$x$$:
\begin{equation}
    \theta = g(w \cdot x + b),
\end{equation}
where $$g$$ is a non-linear function, $$w$$ is a vector and $$b$$ is a scalar. The function $$g$$ is called the *activation function*,
$$w$$ is the *weight vector* and $$b$$ is the *bias*. A simple neural network for binary classification is hence given by

$$\begin{aligned}
    \Pr(y \mid \theta) &= \left\{\begin{array}{ll}
        g(w \cdot x + b) & y = 1\\
        1 - g(w \cdot x + b) & y = 0
    \end{array}\right.
\end{aligned}$$

Obviously, the function $$g$$ should be chosen such that its range is $$[0,1]$$.
Deeper neural networks simply repeat the above pattern: they stack multiple non-linear functions and linear transformations on top of each other.
At the end of the day, we train these networks the same way we trained our linear regression: we derive an expression for the MAP estimate and
try to maximize it. In essence, that's really all there is to it, although constructing the right network for a given job can be very hard.

It should be clear from the above discussions that machine learning is very much an art as well as a science.
There are no clear universal guidelines on how to define your hypothesis space and how to set your prior so the model doesn't overfit.
Yet researchers try, and they have achieved great things so far. One thing I believe is certain: machine learning is not a fad, it is here to stay.