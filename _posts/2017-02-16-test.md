---
title: "What If You Lose Your Test Set?"
layout: post
date: 2017-02-16
---
Suppose you are constructing some machine learning model to solve some task, but for whatever reason you forgot what test set you used to evaluate its performance.
For example, you might have a dataset consisting of $$N$$ i.i.d. samples, which you split up into two disjoint subsets: $$N_1$$ training samples and $$N_2$$ test samples.
The goal of a test set is to see how well a model generalizes to new problem instances, so it is important that the training and test data never overlap; otherwise,
you're testing how well the model can regurgitate what is has seen before. [As was shown recently](https://arxiv.org/pdf/1611.03530.pdf), even shallow neural networks
already have enough capacity to represent almost any arbitrary labelling of inputs, so they will usually be able to memorize a given dataset pretty well.
Hence, if you're evaluating e.g. the accuracy of a machine learning model, you need to test the model on inputs it hasn't seen yet. But what if you forget what samples
the model has been trained on? There are at least two possible solutions that come to mind:

1. Try to obtain new data which you're sure the model hasn't seen before. Depending on your specific task, however, this may be impossible or prohibitively expensive.

2. Test the model on random samples from the dataset and hope this random sample contains enough unseen examples so as to be representative of the model's generalization
capabilities.

Option 1 is pretty straightforward, so I'll focus on option 2.
A reasonable strategy to estimate the actual accuracy $$a$$ on the original test set is to take $$k$$ independent batches of $$N_2$$ random samples from the dataset, and to compute
\begin{equation}
    \hat{a} = \frac{1}{kN_2}\sum_{i=1}^k\sum_{j=1}^{N_2} I(y(x_{ij}) = f(x_{ij})).
\end{equation}
Here, $$I(\phi)$$ is the indicator function, which is equal to 1 when $$\phi$$ is true and 0 otherwise; $$y(x_{ij})$$ is the actual label of sample $$j$$ from batch $$i$$ and
$$f(x_{ij})$$ is the classification of this sample by the model. Does this estimator converge to the true accuracy as $$k$$ approaches $${N \choose N_2}$$?
If $$k$$ is sufficiently large so that there is a high probability of the real test set being among the random batches, we can write
\begin{equation}
    \hat{a} = \frac{a}{k} + \frac{1}{kN_2}\sum_{i=2}^k\sum_{j=1}^{N_2} I(y(x_{ij}) = f(x_{ij})),
\end{equation}
assuming (without loss of generality) that the first batch is the real test set. The approximation error is then given by
\begin{equation}
    \Delta a = \frac{1}{kN_2}\sum_{i=2}^k\sum_{j=1}^{N_2} I(y(x_{ij}) = f(x_{ij})).
\end{equation}
As $$\Delta a \geq 0$$, we have $$\hat{a} \geq a$$, so the estimator yields an upper bound on the true accuracy.
Let $$\varepsilon_1$$ and $$\varepsilon_2$$ denote the training error and test error, respectively.
The probability of a random sample from the dataset being classified correctly is
\begin{equation}
    p = \frac{N_1}{N}(1 - \varepsilon_1) + \frac{N_2}{N}(1 - \varepsilon_2).
\end{equation}
We thus find
\begin{equation}
    E[\Delta a] = \frac{p(k-1)}{k} = p\left(1 - \frac{1}{k} \right).
\end{equation}
Hence
\begin{equation}
    E[\hat{a}] = \frac{a}{k} + p\left(1 - \frac{1}{k} \right).
\end{equation}
One must be careful not to conclude from this that $$E[\hat{a}] = a$$ for $$k=1$$, since the assumption underlying these formulas is that the real test set is among the $$k$$ batches.
This is unlikely if there is only one batch. However, $$k$$ is a parameter of our choosing and $$p$$ can be computed directly from the dataset (it is simply the accuracy on the entire
dataset). Hence, we can compute $$\frac{1}{k}$$ as well as $$p\left(1 - \frac{1}{k} \right)$$. We can then define
\begin{equation}
    \alpha = \frac{1}{k}\left( \hat{a} - p\left(1 - \frac{1}{k} \right) \right).
\end{equation}
This yields
\begin{equation}
    E[\alpha] = \frac{1}{k}\left( E[\hat{a}] - p\left(1 - \frac{1}{k} \right) \right) = a.
\end{equation}
Thus, $$\alpha$$ is an unbiased estimator of the accuracy of our model on its test set.