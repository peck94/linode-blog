---
title: Anomaly Detection in Time Series
date: 2017-08-22
layout: post
tags: ml
---
A problem that crops up frequently in machine learning is the forecasting of time series. Formally, a *time series* is just a sequence of real data points $$x_1, x_2, x_3, \dots \in \mathbb{R}$$ representing some process which evolves with time such as temperature or ECG signals. Forecasting a time series means predicting the value of an (unseen) data point $$x_t$$ given some or all of the previous data points $$x_1, \dots, x_{t-1}$$.

Anomaly detection is related to the problem of forecasting: in anomaly detection, we are interested in whether a part of a time series is "normal" or "abnormal". This could be achieved via forecasting, by first predicting the next part of a time series given the previous data and flagging the new data as anomalous if it deviates too much from the predicted data. In this post, I will describe an attempt at anomaly detection using hidden Markov models (HMMs).

# Hidden Markov models

An HMM is basically a Markov process whose states we cannot directly observe. These so-called "hidden" states produce visible observations which we can use to estimate the hidden variables. Formally, an HMM consists of a *transition model* $$p(z_{t+1} \mid z_t)$$ along with an *observation model* $$p(x_t \mid z_t)$$. The transition model determines the actual Markov process: it describes how the hidden states evolve over time. The observation model determines which observations can result from a given hidden state, and with what probability. The full joint probability of an HMM is given by

$$\begin{equation}\label{eq:joint}
    p(x_{1:T}, z_{1:T}) = \left( p(z_1)\prod_{t=2}^Tp(z_t \mid z_{t-1}) \right)\prod_{t=1}^Tp(x_t \mid z_t).
\end{equation}$$

Note that we assume the Markov process is stationary, i.e. $$p(z_{t+1} \mid z_t)$$ is independent of $$t$$. This may not be a realistic assumption in many cases, but taking non-stationarity into account would overly complicate this simple blog post.

# Anomaly detection

In order to use HMMs for anomaly detection, we will assume the hidden states can take on $$K$$ different values, i.e. $$z_t \in \{1, \dots, K\}$$. An "anomaly" is then formally defined as a transition between different hidden states. That is, we say an anomaly occurs in the time series at time $$t$$ if $$z_{t-1} \not= z_t$$.

In order to fit the full joint probability \eqref{eq:joint}, we first choose a prior on $$z_1$$, which will be a categorical distribution:

$$
    p(z_1) = \mathrm{Cat}(\vec{\pi}).
$$

Then, we need to specify the conditional distribution $$p(z_{t+1} \mid z_t)$$. Since $$z_t$$ is discrete and finite, this may be done by specifiying a matrix $$A$$ where $$p(z_{t+1} = j \mid z_t = i) = a_{ij}$$. Given that $$p(z_1)$$ is categorically distributed, we have

$$
    p(z_{t+1}) = \vec{\pi}^\top A^t.
$$

Lastly, we need to specify the observation model $$p(x_t \mid z_t)$$. The $$x_t$$ form our time series, so they will be real numbers. Hence $$p(x_t \mid z_t)$$ should be some univariate continuous distribution. The simplest choice is the Gaussian:

$$
    p(x_t \mid z_t = k) = \mathcal{N}(x_t \mid \mu_k, \sigma_k^2).
$$

# Parameter estimation

Now that the model is fully specified, it remains to estimate the parameters from the data. In the simplest case, we have a data set $$\mathcal{D} = \{ (x_i, z_i) \mid i = 1, \dots, N \}$$ of training samples where the hidden states are supplied. In practice, however, the $$z_i$$ will rarely (if ever) be given to us, since the whole point of using machine learning here is to be able to estimate the $$z_i$$ *without* having ever seen them explicitly spelled out. This may be done using general-purpose optimization techniques or, preferably, the [Baum-Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm).

Of course, for these techniques to work, we need to specify a value for $$K$$. A good estimate for $$K$$ can be obtained via [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).

# State estimation

Finally, to detect possible anomalies, we estimate $$p(z_t \mid x_{1:t})$$ for each $$t$$ and determine the most likely assignment:

$$
    \hat{z}_t = \arg\underset{k}{\max} p(z_t = k \mid x_{1:t}).
$$

This can be done using the [forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm). We then scan our estimates $$\hat{z}_1, \hat{z}_2, \dots$$. If at any point $$t > 1$$ we find that $$\hat{z}_t \not= \hat{z}_{t-1}$$, we signal an anomaly at time $$t$$.

# Experiments

We will now test the above model on a time series data set, namely the [IBM common stock closing prices](https://datamarket.com/data/set/231i/ibm-common-stock-closing-prices). An export of the data in CSV format can be found [here](/assets/files/ibm.csv). Figure 1 shows a plot of the entire time series, which starts at 2 January 1962 and ends at 31 December 1965.

<figure>
    <center>
        <img src="/assets/img/ibm.png" width="500px">
        <br>
        <caption>Figure 1. Plot of the IBM common stock closing prices</caption>
    </center>
</figure>

To fit our model on this data set, we will use the [hmmlearn](http://hmmlearn.readthedocs.io) Python package, which provides a convenient Python API for estimating and sampling from HMMs.

<figure>
    <center>
        <img src="/assets/img/ibm_an.png" width="500px">
        <br>
        <caption>Figure 2. Anomalies detected in IBM common stock closing prices</caption>
    </center>
</figure>

Setting $$K = 3$$ (which looked like the best choice) we get the results from Figure 2. The anomalies are marked with red lines. The model has found the following means and variances for the Gaussians:

$$\begin{aligned}
    \mu_1 &= 400 & \mu_2 &= 466 & \mu_3 &= 541\\
    \sigma_1^2 &= 907 & \sigma_2^2 &= 225 & \sigma_3^2 &= 776
\end{aligned}$$

The following initial probabilities:

$$
    \vec{\pi} = [ 1, 0, 0 ]
$$

And the following transition matrix:

$$
    A = \left[\begin{matrix}
        0.99 & 0 & 0.01\\
        0 & 0.99 & 0.01\\
        0.01 & 0.01 & 0.98
    \end{matrix}\right]
$$

We can see from the transition matrix that anomalies (i.e. state transitions) are indeed rare, as the diagonal entries are all greater than 98%. The three hidden states appear to correspond to periods of low ($$\mu_1$$), medium ($$\mu_2$$) and high ($$\mu_3$$) stock prices. The low and high periods also have the highest variance, so they are most "unstable"; the medium period has significantly lower variance.

We can also see from the transition matrix that the high period is ever so slightly less likely to last than the others, since the probability of remaining in the high state is 98% whereas this probability is 99% for the low and medium states. Some transitions appear to never even happen at all, such as going from low to medium or from medium to low. From a high state we can transition to any one of the other states, but from a low state we can only transition to low or high and from a medium state we can only transition medium or high. This means stock prices can be low at one point and suddenly jump up dramatically the next or vice versa, which is reminiscent of so-called "boom-and-bust" cycles. Although one should not forget that the probability of any transition is consistently at most 1% according to this model...

<table>
    <caption>Table 1. Predicted counts of hidden states</caption>
    <tr>
        <th></th>
        <th>Count</th>
        <th>Percentage</th>
    </tr>
    <tr>
        <td>Low</td>
        <td>353</td>
        <td>35%</td>
    </tr>
    <tr>
        <td>Medium</td>
        <td>268</td>
        <td>27%</td>
    </tr>
    <tr>
        <td>High</td>
        <td>387</td>
        <td>38%</td>
    </tr>
</table>

The counts of the different hidden states are predicted as in Table 1. The high state appears to be most common (38%), followed by low (35%) and finally medium (27%). Hence the stock prices are usually either low or high and least often medium. Together with the above findings, we are indeed again reminded of the boom-and-bust cycles:

* the stock prices are usually either low or high;
* if prices are low, they can transition to high at some point in the future but not to medium;
* if prices are medium, they can transition to high in the future but not to low;
* if prices are high, they can transition to medium or low in the future.

The fact that low prices can only transition to high mimicks the boom part of the cycle, whereas the transition of high to low mimicks the bust part. Busts may vary in severity, however, so high prices may also transition to medium and then back to high.

# Conclusions

Hidden Markov models have various uses. In this post, I demonstrated only one: modelling of time series with the aim of detecting anomalies. HMMs are, however, also used for things like speech recognition. We have also seen that HMMs can lend themselves nicely to human interpretation, a property which is hard to come by in the age of deep black-box neural networks.

One should also not take the economical implications of the above analysis too seriously. It is important to note that the assumptions made in the HMM heavily influence the conclusions, and it is unlikely that the economy evolves according to a stationary Gaussian with first-order stationary Markovian dynamics. This may be somewhat true for the limited period of 1962-1965, but it is doubtful these assumptions hold in the long term.

Finally, it may be interesting to consider the question of how to model multivariate time series, i.e. time series whose value at time $$t$$ is a real vector $$\vec{x}_t \in \mathbb{R}^d$$ instead of just a real number $$x_t \in \mathbb{R}$$. The extension is rather obvious: one simply fits a multivariate continuous distribution to $$p(\vec{x}_t \mid z_t)$$ instead of a univariate one. This complicates training, of course, but it can be done in principle.
