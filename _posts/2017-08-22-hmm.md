---
title: Anomaly Detection in Time Series
date: 2017-08-22
layout: post
tags: ml
---
A problem that crops up frequently in machine learning is the forecasting of time series. Formally, a *time series* is just a sequence of real data points $$x_1, x_2, x_3, \dots \in \mathbb{R}$$ representing some process which evolves with time such as temperature or ECG signals. Forecasting a time series means predicting the value of an (unseen) data point $$x_t$$ given some or all of the previous data points $$x_1, \dots, x_{t-1}$$.

Anomaly detection is related to the problem of forecasting: in anomaly detection, we are interested in whether a part of a time series is "normal" or "abnormal". This could be achieved via forecasting, by first predicting the next part of a time series given the previous data and flagging the new data as anomalous if it deviates too much from the predicted data. In this post, I will describe an attempt at anomaly detection using hidden Markov models (HMMs).

# Hidden Markov models

An HMM is basically a Markov process whose states we cannot directly observe. These so-called "hidden" states produce visible observations which we can use to estimate the hidden variables. Formally, an HMM consists of a *transition model* $$p(z_{t+1} \mid z_t)$$ along with an *observation model* $$p(x_t \mid z_t)$$. The transition model determines the actual Markov process: it describes how the hidden states evolve over time. The observation model determines which observations can result from a given hidden state, and with what probability. The full joint probability of an HMM is given by

$$\begin{equation}\label{eq:joint}
    p(x_{1:T}, z_{1:T}) = \left( p(z_1)\prod_{t=2}^Tp(z_t \mid z_{t-1}) \right)\prod_{t=1}^Tp(x_t \mid z_t).
\end{equation}$$

Note that we assume the Markov process is stationary, i.e. $$p(z_{t+1} \mid z_t)$$ is independent of $$t$$. This may not be a realistic assumption in many cases, but taking non-stationarity into account would overly complicate this simple blog post.

# Anomaly detection

In order to use HMMs for anomaly detection, we will assume the hidden states can take on $$K$$ different values, i.e. $$z_t \in \{1, \dots, K\}$$. An "anomaly" is then formally defined as a transition between different hidden states. That is, we say an anomaly occurs in the time series at time $$t$$ if $$z_{t-1} \not= z_t$$.

In order to fit the full joint probability \eqref{eq:joint}, we first choose a prior on $$z_1$$, which will be a categorical distribution:

$$
    p(z_1) = \mathrm{Cat}(\vec{\pi}).
$$

Then, we need to specify the conditional distribution $$p(z_{t+1} \mid z_t)$$. Since $$z_t$$ is discrete and finite, this may be done by specifiying a matrix $$A$$ where $$p(z_{t+1} = j \mid z_t = i) = a_{ij}$$. Given that $$p(z_1)$$ is categorically distributed, we have

$$
    p(z_{t+1}) = \vec{\pi}^\top A^t.
$$

Lastly, we need to specify the observation model $$p(x_t \mid z_t)$$. The $$x_t$$ form our time series, so they will be real numbers. Hence $$p(x_t \mid z_t)$$ should be some univariate continuous distribution. The simplest choice is the Gaussian:

$$
    p(x_t \mid z_t = k) = \mathcal{N}(x_t \mid \mu_k, \sigma_k^2).
$$

# Parameter estimation

Now that the model is fully specified, it remains to estimate the parameters from the data. In the simplest case, we have a data set $$\mathcal{D} = \{ (x_i, z_i) \mid i = 1, \dots, N \}$$ of training samples where the hidden states are supplied. In practice, however, the $$z_i$$ will rarely (if ever) be given to us, since the whole point of using machine learning here is to be able to estimate the $$z_i$$ *without* having ever seen them explicitly spelled out. This may be done using general-purpose optimization techniques or, preferably, the [Baum-Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm).

Of course, for these techniques to work, we need to specify a value for $$K$$. A good estimate for $$K$$ can be obtained via [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).

# State estimation

Finally, to detect possible anomalies, we estimate $$p(z_t \mid x_{1:t})$$ for each $$t$$ and determine the most likely assignment:

$$
    \hat{z}_t = \arg\underset{k}{\max} p(z_t = k \mid x_{1:t}).
$$

This can be done using the [forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm). We then scan our estimates $$\hat{z}_1, \hat{z}_2, \dots$$. If at any point $$t > 1$$ we find that $$\hat{z}_t \not= \hat{z}_{t-1}$$, we signal an anomaly at time $$t$$.

# Experiments

We will now test the above model on a time series data set, namely the [IBM common stock closing prices](https://datamarket.com/data/set/231i/ibm-common-stock-closing-prices). An export of the data in CSV format can be found [here](/assets/files/ibm.csv). Figure 1 shows a plot of the entire time series, which starts at 2 January 1962 and ends at 31 December 1965.

<figure>
    <center>
        <img src="/assets/img/ibm.png" width="500px">
        <br>
        <caption>Figure 1. Plot of the IBM common stock closing prices</caption>
    </center>
</figure>

To fit our model on this data set, we will use the [hmmlearn](http://hmmlearn.readthedocs.io) Python package, which provides a convenient Python API for estimating and sampling from HMMs.

<figure>
    <center>
        <img src="/assets/img/ibm_an.png" width="500px">
        <br>
        <caption>Figure 2. Anomalies detected in IBM common stock closing prices</caption>
    </center>
</figure>

Setting $$K = 3$$ (which looked like the best choice) we get the results from Figure 2. The anomalies are marked with red lines. The model has found the following means and variances for the Gaussians:

$$\begin{aligned}
    \mu_1 &= 400 & \mu_2 &= 466 & \mu_3 &= 541\\
    \sigma_1^2 &= 907 & \sigma_2^2 &= 225 & \sigma_3^2 &= 776
\end{aligned}$$

The following initial probabilities:

$$
    \vec{\pi} = [ 1, 0, 0 ]
$$

And the following transition matrix:

$$
    A = \left[\begin{matrix}
        0.99 & 0 & 0.01\\
        0 & 0.99 & 0.01\\
        0.01 & 0.01 & 0.98
    \end{matrix}\right]
$$

We can see from the transition matrix that anomalies (i.e. state transitions) are indeed rare, as the diagonal entries are all greater than 98%.
