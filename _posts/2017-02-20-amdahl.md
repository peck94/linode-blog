---
title: "Amdahl's Law"
layout: post
date: 2017-02-20
tags: complexity
---
In parallel computing, one often faces the following question:

>At what point does further parallellization stop benefiting the performance of my system?

Amdahl's Law provides an answer to this problem. The idea is that there is a fixed workload which must be done by our program,
and our program consists of serial portions which cannot be parallellized at all as well as portions which can be divided up and parallellized infinitely.
Let $$s$$ be the serial fraction of the program, then $$1-s$$ is the fraction that can be parallellized. Let $$T_0$$ be the time our program takes to
solve its task when no parallellization is done and denote by $$T(n)$$ the time our program takes when the parallellizable fraction is spread out evenly
over $$n$$ processing units. Amdahl's Law then states:

$$
    T(n) = sT_0 + (1-s)\frac{T_0}{n} = T_0\left( s + \frac{1-s}{n} \right).
$$

The speedup is given by

$$
    S(n) = \frac{T_0}{T(n)} = \left( s + \frac{1-s}{n} \right)^{-1}
$$

The curious thing about Amdahl's Law is it predicts that

$$
    \underset{n \to \infty}{\lim}S(n) = \frac{1}{s},
$$

meaning there is an upper bound on the speedup one can get when introducing new nodes.
However, one detail that Amdahl's Law does not take care of is the cost of adding new nodes to the system.
Amdahl's Law assumes that there is no overhead whatsoever when passing data around between different nodes.
There do exist tasks which require absolutely no communication between the nodes (such tasks are called *embarassingly parallel*), but there will always
be at least some communication and therefore at least some overhead. If this overhead can be measured in computing time, then we can adapt Amdahl's Law
so that it incorporates an arbitrary overhead function $$f$$:

$$
    T(n) = T_0\left( s + \frac{1-s}{n} \right) + f(n).
$$

Our speedup then becomes

$$
    S(n) = \frac{T_0}{T(n)} = \left( s + \frac{1-s}{n} + \frac{f(n)}{T_0} \right)^{-1}.
$$

This yields

$$
    \frac{\partial}{\partial n}S(n) = -\frac{1}{S(n)^2}\left( \frac{f^\prime(n)}{T_0} - \frac{1-s}{n^2} \right)
$$

The optimal choices for $$n$$ are thus given by

$$
    n^2f^\prime(n) = (1-s)T_0.
$$

Suppose, as might be reasonable in some cases, that our overhead is linear in $$n$$:

$$
    f(n) = an + b,
$$

where $$a > 0$$. We then find the optimal $$n$$ to be the solution of

$$
    n^2a = (1-s)T_0.
$$

This yields

$$
    n = \sqrt{\frac{1-s}{a}T_0}.
$$

If our overhead is logarithmic in $$n$$,

$$
    f(n) = \log n,
$$

then we find the optimal $$n$$ to be

$$
    n = (1-s)T_0.
$$

For any constant overhead $$f(n) = c$$, however, there is no optimal choice for $$n$$.
The original formulation of Amdahl's Law assumes $$f(n) = 0$$, so it cannot give an optimal choice for $$n$$.
It can predict that the speedup is upper-bounded, though, by taking $$n \to \infty$$.
If we take $$n$$ equal to its optimal value and we compute the speedup for optimal $$n$$, then we get the highest attainable speedup with the given overhead.
For example, if we have a linear overhead $$f(n) = 10n + 3$$, a serial portion of $$s = 0.9$$ and an initial time $$T_0 = 500$$ seconds, then the optimal $$n$$ is

$$
    n = \sqrt{\frac{0.1}{10} \cdot 500} \approx 2.
$$

Obviously, we have to round to the nearest integer. We then have

$$
    S(2) \approx 1.004.
$$