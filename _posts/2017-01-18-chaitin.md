---
title: "Chaitin's Constant"
date: 2017-01-18
layout: post
tags: complexity
---
For those of you who paid attention to the favicon of this blog, you might have noticed it is the capital letter omega, $$\Omega$$.
More specifically, it is a stylized version of this letter composed of ones and zeroes:

![omega](/assets/img/favicon.png){: .center_image }

You may wonder what this image represents. Well, it is *Chaitin's constant*, and the reason I picked it for my favicon is because this is a very interesting constant indeed.
It is also dubbed the *halting probability*, as it can be interpreted as giving the probability that an arbitrary Turing machine will halt on an empty input. Its inventor,
Gregory Chaitin, wrote a whole book basically about this constant, *Algorithmic Information Theory*. The field of algorithmic information theory is itself fascinating, but
here I'll just stick to Chaitin's constant for now.

In order to define $$\Omega$$, we'll use the Turing machine as our model of computation. Then, we will assume we have a partial function $$U$$ that satisfies the following properties:

1. It is a function from bit strings to bit strings.

2. It is *universal*, ie for every computable function $$f$$ there exists a bit string $$w$$ such that for all bit strings $$x$$, $$U(wx) = f(x)$$. That is, we can interpret $$w$$ as a description
of $$f$$ as a bit string and $$U$$ as a program that reads this description and then simulates $$f$$ on input $$x$$.

3. It is *computable*, ie there exists a Turing machine that computes $$U$$.

The *domain* of $$U$$ is the set of bit strings on which it is defined. Furthermore, we can assume this domain is *prefix-free*: no bit string from the domain of $$U$$ is a prefix of another bit string in the
domain. The reason for requiring the domain of $$U$$ to be prefix-free is so we can apply Kraft's inequality. This inequality states that, whenever $$S$$ is a prefix-free set over an alphabet of size $$r$$,
we have
\begin{equation}
    \sum_{s \in S}r^{-|s|} \leq 1.
\end{equation}
In our case, the domain of $$U$$ is a prefix-free set of bit strings, so $$r = 2$$ and we can define
\begin{equation}
    \Omega = \sum_{p \in \mathrm{dom}\,U}2^{-|p|}.
\end{equation}
Thanks to Kraft's inequality, we know this is a real number somewhere between 0 and 1. This is Chaitin's constant, although there are infinitely many such constants depending on what choice of $$U$$ you make.
Chaitin's constant is simply any number which satisfies the properties outlined above for suitable choices of $$U$$.

Why is this constant so fascinating? At first glance, it doesn't appear to be particularly interesting; it looks like just another contrived mathematical curiosity. However, being able to actually
compute Chaitin's constant would not only be very interesting for academic purposes, it would be of very high practical relevance as well. To illustrate, I will prove one of the most important properties
of this constant:

**Theorem.** Suppose $$\Omega$$ is computable. Then, for any Turing machine $$M$$, we can decide whether or not $$M$$ halts.

*Proof.* The basic idea of this proof is to first encode $$M$$ as a binary string $$w$$ fit for $$U$$ and compute the first $$|w|$$ bits of $$\Omega$$. Then,
we run all programs of all possible lengths simultaneously using a technique called [dovetailing](https://en.wikipedia.org/wiki/Dovetailing_(computer_science)).
This technique boils down to the following steps:

1. Order all programs lexicographically according to their binary encodings.

2. Run the first step of the first program.

3. Run the second step of the first program and the first step of the second program.

4. Run the third step of the first program, the second step of the second program and the first step of the third program.

5. etc.

Each time a program of length $$p$$ halts, we add $$2^{-p}$$ to a counter that was initially zero. Clearly, this counter converges towards $$\Omega$$ in the sense that after some finite amount of time,
the first $$n$$ bits of the counter will equal the first $$n$$ bits of $$\Omega$$ for any $$n$$. Since we have already computed the first $$|w|$$ bits of $$\Omega$$, we know what they should look
like and we will know when our counter has successfully computed those bits. When we detect that it has, we need only check if $$M$$ has halted by now. Since the encoding of $$M$$ has length $$|w|$$,
it can only affect the first $$|w|$$ bits of $$\Omega$$. Once these bits are fixed, if $$M$$ hasn't halted yet, it never will. $$\square$$

Why is this so impressive? Well, as detailed in [this paper](http://www.scottaaronson.com/busybeaver.pdf), if we could decide the halting problem then we could also solve [Goldbach's conjecture](https://en.wikipedia.org/wiki/Goldbach's_conjecture)
or even [the Riemann hypothesis](https://en.wikipedia.org/wiki/Riemann_hypothesis). More generally, if $$H$$ is any hypothesis whose truth can be reduced to the halting of some Turing machine, then $$H$$ could
be solved by computing enough bits of $$\Omega$$.

Perhaps a simpler but ultimately equivalent notion is the following. Suppose we have some way to encode descriptions of Turing machines as binary strings, then we can impose a total order on these strings
using the conventional lexicographic order. We can then create a real number $$0.r_1r_2r_3\dots$$ satisfying the following property for all $$i$$:

$$\begin{aligned}
    r_i = \left\{\begin{array}{ll}
        1 & \mbox{if the $i$th Turing machine halts on an empty input}\\
        0 & \mbox{otherwise}
    \end{array}\right..
\end{aligned}$$

This number basically holds the answer to every decision problem ever. In a sense, this number is the answer to life, the universe and everything. But of course, we can't compute it.
