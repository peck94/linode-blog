---
layout: post
title: "The AI Threat"
date: 2017-01-28
---
As with the rise of any new technology, the increasing success of artificial intelligence has sparked scepticism and is shrouded in controversy in many ways.
To be sure, almost nobody objects to the use of AI for speech and image recognition or other "run of the mill" applications which currently dominate the field.
The concerns of "AI alarmists" (as I have taken to calling them) can be split into two categories:

1. *Short-term concerns*. These are mostly about applications of AI to areas previously thought accessible only to humans, such as self-driving cars and autonomous weapons.
This class of concerns is the most pressing, since they apply to developments in AI which are already underway.

2. *Long-term concerns*. These focus on potential future applications of AI, mainly the concept of an "Artificial General Intelligence" (AGI).
This class is less relevant at the moment, since there is currently insufficient evidence to determine when (and even if) anything like an AGI will ever be developed, let
alone what its precise form and context will be once it is.

I will address both of these classes of concerns in this article. For simplicity, I will refer to all applications of AI which raise short-term concerns as "Class 1 problems".
All applications which raise long-term concerns will be dubbed "Class 2 problems".

# Short-term concerns

In my opinion, the short-term concerns about the safety of applications such as autonomous vehicles are the most realistic and most worthy of being discussed.
The simple fact of the matter is that there are legitimate issues with these inventions. However, I will argue the following:

>The issues surrounding Class 1 problems are often overstated.
Moreover, they are inherent to technological evolution in general and thus have come up and been dealt with before.

The most common concerns raised with regard to Class 1 applications of AI are the inevitable job losses resulting from automation and ethical issues regarding decisions AIs will
have to make once widely deployed. To be sure, these are both very valid concerns: when someone's job is automated, that job is of course gone; when an autonomous vehicle has to
choose between killing the family of four it is carrying by slamming into a wall or potentially killing many more people on board of the bus it is otherwise going to crash into,
that is an ethical decision the AI has to make. But these issues are fundamentally misunderstood by most people discussing them. For one, the job losses resulting from increased
automation have been part of the industrial revolution since the 1800s. Not only is this problem not inherent to AI, it also does not appear to be a very critical one.
As far as I know, the economic literature surrounding so-called "technological unemployment" (which is unemployment resulting from automation) is quite unanimous in its condemnation
of this sort of thinking. An economy does not have a fixed amount of jobs (this is known as the fixed pie fallacy), and it has been observed time and again throughout the technological
development of mankind that people whose jobs are automated eventually transition to other sectors where demand for jobs has risen in response to the increased automation.
Automating away certain jobs makes the sector as a whole more efficient (otherwise the automation would not have taken place), which in turn stimulates demand in other sectors.
The details of this are beyond my capacity to fully explain as I am no economist, so I will end the discussion of this point here. Suffice it to say, the economic consensus as I
understand it is that automation has never resulted in significant, sustained unemployment, and so should not be treated as some new kind of catastrophe that will ruin us all.
It is a problem we have had to deal with for several hundred years already, and there is no reason why it should be any different this time around.

There remain, of course, the ethical decisions AIs will have to be programmed for. I am being very generous here: I assume a Class 1 AI such as a self-driving car will be able
to determine when it has to make these decisions, which is suspect at best. But even granting this ability to the AI, it is quite curious that people are only now making this into a
significant problem, and apparently believe it is one which only affects the AI. Don't we humans face the same problems every single day? Take the following popular example: you
have a self-driving car which, due to unforeseen circumstances, has to make an emergency stop. However, it is too short notice: the car is simply physically incapable of stopping in
time, so it is going to crash regardless of what happens next. The AI must decide whether to divert the car off the road and slam into a nearby wall, potentially killing everyone on
board, or keep going forward and crash into a bus right in front of the car, potentially killing many more people on board the bus as well as everyone in the car. This is simply a
version of the trolley problem adapted to the situation of a self-driving car. If we grant that these things may occur in real life (which is not unrealistic), it suffers from an
analogous objection as the technological unemployment: it is not unique to AI. The trolley problem is not an argument against AI *per se* and should not be used as such; rather, if one is to
take this argument seriously, one has to admit it applies just as well to human drivers as it does to artificial ones. Yet humans are allowed to drive cars anyway, which is curious
to say the least. To summarize, one cannot object to Class 1 AI applications based on ethical issues and simultaneously hold that humans should be allowed to continue practicing
these Class 1 problems anyway. Either these ethical objections are grounds for deeming no one, neither human nor AI, fit for Class 1 problems or they are grounds for treating AIs
in exactly the same way we treat humans. In the case of humans, there is a very *laissez-faire* attitude. For example, to obtain a driver's licence, one usually has to pass a
theoretical and a practical test. After that, one is free to drive pretty much anywhere without strict supervision by some Hobbesian Leviathan. Why not do the same exact thing for
a self-driving car? Subject an implementation of the AI to a practical test analogous to the one humans have pass to obtain their licence. This would be some sort of a Turing test:
if the AI can drive at least as well as can be expected from an average human, then what more do you want? It is completely arbitrary and borderline hypocritical to hold AIs to
higher standards than humans when these AIs are designed to perform tasks humans are entrusted with every day. It simply makes no sense except from a radical Luddite perspective
which does not want to see these AIs put into practice at all regardless of their performance.