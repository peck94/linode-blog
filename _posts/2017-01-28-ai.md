---
layout: post
title: "The AI Threat"
date: 2017-01-28
tags: rant
---
As with the rise of any new technology, the increasing success of artificial intelligence has sparked scepticism and is shrouded in controversy in many ways.
To be sure, almost nobody objects to the use of AI for speech and image recognition or other "run of the mill" applications which currently dominate the field.
The concerns of "AI alarmists" (as I have taken to calling them) can be split into two categories:

1. *Short-term concerns*. These are mostly about applications of AI to areas previously thought accessible only to humans, such as self-driving cars and autonomous weapons.
This class of concerns is the most pressing, since they apply to developments in AI which are already underway.

2. *Long-term concerns*. These focus on potential future applications of AI, mainly the concept of an "Artificial General Intelligence" (AGI).
This class is less relevant at the moment, since there is currently insufficient evidence to determine when (and even if) anything like an AGI will ever be developed, let
alone what its precise form and context will be once it is.

I will address both of these classes of concerns in this article. For simplicity, I will refer to all applications of AI which raise short-term concerns as "Class 1 problems".
All applications which raise long-term concerns will be dubbed "Class 2 problems".

# Short-term concerns

In my opinion, the short-term concerns about the safety of applications such as autonomous vehicles are the most realistic and most worthy of being discussed.
The simple fact of the matter is that there are legitimate issues with these inventions. However, I will argue the following:

>The issues surrounding Class 1 problems are often overstated.
Moreover, they are inherent to technological evolution in general and thus have come up and been dealt with before.

The most common concerns raised with regard to Class 1 applications of AI are the inevitable job losses resulting from automation and ethical issues regarding decisions AIs will
have to make once widely deployed. To be sure, these are both very valid concerns: when someone's job is automated, that job is of course gone; when an autonomous vehicle has to
choose between killing the family of four it is carrying by slamming into a wall or potentially killing many more people on board of the bus it is otherwise going to crash into,
that is an ethical decision the AI has to make. But these issues are fundamentally misunderstood by most people discussing them. For one, the job losses resulting from increased
automation have been part of the industrial revolution since the 1800s. Not only is this problem not inherent to AI, it also does not appear to be a very critical one.
As far as I know, the economic literature surrounding so-called "technological unemployment" (which is unemployment resulting from automation) is quite unanimous in its condemnation
of this sort of thinking. An economy does not have a fixed amount of jobs (this is known as the fixed pie fallacy), and it has been observed time and again throughout the technological
development of mankind that people whose jobs are automated eventually transition to other sectors where demand for jobs has risen in response to the increased automation.
Automating away certain jobs makes the sector as a whole more efficient (otherwise the automation would not have taken place), which in turn stimulates demand in other sectors.
The details of this are beyond my capacity to fully explain as I am no economist, so I will end the discussion of this point here. Suffice it to say, the economic consensus as I
understand it is that automation has never resulted in significant, sustained unemployment, and so should not be treated as some new kind of catastrophe that will ruin us all.
It is a problem we have had to deal with for several hundred years already, and there is no reason why it should be any different this time around.

There remain, of course, the ethical decisions AIs will have to be programmed for. I am being very generous here: I assume a Class 1 AI such as a self-driving car will be able
to determine when it has to make these decisions, which is suspect at best. But even granting this ability to the AI, it is quite curious that people are only now making this into a
significant problem, and apparently believe it is one which only affects the AI. Don't we humans face the same problems every single day? Take the following popular example: you
have a self-driving car which, due to unforeseen circumstances, has to make an emergency stop. However, it is too short notice: the car is simply physically incapable of stopping in
time, so it is going to crash regardless of what happens next. The AI must decide whether to divert the car off the road and slam into a nearby wall, potentially killing everyone on
board, or keep going forward and crash into a bus right in front of the car, potentially killing many more people on board the bus as well as everyone in the car. This is simply a
version of the trolley problem adapted to the situation of a self-driving car. If we grant that these things may occur in real life (which is not unrealistic), it suffers from an
analogous objection as the technological unemployment: it is not unique to AI. The trolley problem is not an argument against AI *per se* and should not be used as such; rather, if one is to
take this argument seriously, one has to admit it applies just as well to human drivers as it does to artificial ones. Yet humans are allowed to drive cars anyway, which is curious
to say the least. To summarize, one cannot object to Class 1 AI applications based on ethical issues and simultaneously hold that humans should be allowed to continue practicing
these Class 1 problems anyway. Either these ethical objections are grounds for deeming no one, neither human nor AI, fit for Class 1 problems or they are grounds for treating AIs
in exactly the same way we treat humans. In the case of humans, there is a very *laissez-faire* attitude. For example, to obtain a driver's licence, one usually has to pass a
theoretical and a practical test. After that, one is free to drive pretty much anywhere without strict supervision by some Hobbesian Leviathan. Why not do the same exact thing for
a self-driving car? Subject an implementation of the AI to a practical test analogous to the one humans have pass to obtain their licence. This would be some sort of a Turing test:
if the AI can drive at least as well as can be expected from an average human, then what more do you want? It is completely arbitrary and borderline hypocritical to hold AIs to
higher standards than humans when these AIs are designed to perform tasks humans are entrusted with every day. It simply makes no sense except from a radical Luddite perspective
which does not want to see these AIs put into practice at all regardless of their performance.

# Long-term concerns

The only real long-term concern involves the creation of an AGI, so I will take this as the representative of the Class 2 AI applications.
An AGI has been described as follows:

>Artificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can.

Usually, this is extended to some form of Superintelligence which is capable of self-improvement and problem-solving at a scale physically unattainable for humans.
The fear surrounding such an AGI is the idea that it will spiral out of control and use its superintelligence to wipe out humanity, either accidentally or purposefully.
Almost every single Hollywood movie which features AI comes to mind, which is probably the main source of inspiration for people raising concerns about Class 2.
To be fair, these concerns inhabit a spectrum which ranges from fears that we will literally give birth to Skynet and Terminators to fears that we will create a neutral AI
which just happens to not care about humanity. The former tend to be tin-foil hat conspiracy nuts whose opinion is of little consequence; the latter group, however, includes
people such as Stephen Hawking and Elon Musk, who are influential public figures and forces to be reckoned with. They all seem to share a single characteristic, though, which
is quite damning to their arguments: **they are not AI researchers; they are not even computer scientists**. This holds true for most people that belong to the category of
"Class 2 alarmists": either they are not computer scientists or they are at the very least not AI researchers. Granted, this is a bit of an appeal to authority, but when
speaking of very specialized subject matter such as artificial intelligence (or more generally any specialized scientific topic), the opinion of non-experts should be taken
with a large grain of salt. Contrast this with the opinions of actual AI researchers and you will find they are almost always either agnostic or cautiously optimistic; but
they most definitely never voice such strong opinions regarding this subject as Hawking or Musk do, which is a major red flag that perhaps these non-experts simply don't
fully understand what they are talking about.

But let us suppose for a moment that such an AGI could be built. What would it look like?
The research done by Jürgen Schmidhuber on the [Gödel Machine](http://people.idsia.ch/~juergen/goedelmachine.html) (GM) provides insight into this question:

>Goedel machines are self-referential universal problem solvers making provably optimal self- improvements.
[...]
 Inspired by Kurt Gödel's celebrated self-referential formulas (1931), a Gödel machine (or "Goedel machine" but not "Godel machine")
 rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function
 and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code.

In layman's terms, this is precisely what one would call a self-improving general AI: given any mathematically formalizable problem whatsoever, a GM will find an optimal
solution to it. While it is searching for a solution, it will modify its own mechanism to facilitate this search. It literally self-improves while searching for a solution
to whatever problem we pose it (with the constraint that the problem must have a solution which can be mathematically proven to be optimal).
So suppose sometime in the future an efficient implementation of the GM is created (this does not exist yet at the time of this writing). By all accounts, this should constitute
the creation of an AGI. Note, however, the following important properties:

1. A GM has no personal subjective values. It has only its utility function, which is supplied by the programmer.

2. A GM searches for solutions within a problem space the programmer defines.

Hence, not only should we be able to control exactly what it is an AGI strives to optimize by controlling its utility function, we can also define precisely what its problem space is.
Suppose an AGI is given access to tools which could potentially end all human life, such as a nuclear arsenal, perhaps because we want it to determine how best to manage such an arsenal.
What's to stop the AGI from destroying us all? The fact that we can programmatically disallow it from doing so. An AGI will still be a piece of software consisting of code written by
humans and it will function exactly as its code dictates. Granted, as is common in machine learning, we might not fully understand how and why the code works; but we can perfectly control
what kind of solutions an AGI is capable of enacting. If we do not want to risk it setting off a bunch of nuclear bombs, we simply do not give it the physical and/or programmatic ability to do so.
This could involve barring access to certain peripherals the AI requires to execute this sort of order, or simply not including the undesired behavior in the space of possible solutions.
Or, even better, only program the AI to find a solution and output it in some human-readable format on a computer screen or in a text report and just not give it the ability to manipulate anything
in the real world at all.

Of course, a different question may be posed to counter this pragmatism. Suppose we admit that an AGI could be controlled; what's to stop other malicious actors from creating an AGI
which cannot be controlled on purpose? Terrorist attacks come to mind. But again, this objection suffers from what I can only describe as "AI particularism": it raises an objection
meant to point out some undesired outcome of AI deployment, yet it does not realize this very same objection applies directly to humans as well. What's to stop terrorism in general?
If a malicious group of terrorists ever got hold of a nuclear arsenal, the results could be as disastrous as anything an uncontrolled AGI could ever do to us. Yet we go about our lives
anyway, realizing that these are simply inherent risks we have to take in life. One may object to this answer as well, citing that we have a *choice* in this matter: we can *choose*
whether or not to create an AGI; we do not have a choice when it comes to terrorism. This is false: scientific progress cannot be controlled. If one nation is to outlaw the creation
of an AGI, or even if every single nation on earth is to outlaw it, what's to stop a group of researchers from continuing to work on this in secret? Terrorists do things like that all the
time; the illegality of their actions does not seem a very effective deterrent. Especially when some invention has sufficient military potential, there will be some group somewhere in the world
working to make it a reality regardless of what any law has to say about it. This is an unfortunate but undeniable reality.


# Conclusion

After such a wall of text, it makes sense to provide a summary, a TL;DR if you will.

Regarding Class 1 AI applications, it is my opinion that the issues surrounding them are often overstated, and most objections suffer from "AI particularism":
they apply to humans just the same, yet this does not seem to be a problem or is at the very least a problem which has proven manageable. There is no reason
why the exact same problems can't be just as manageable when it comes to AI.

Regarding Class 2 AI applications such as AGI, I believe that, if an AGI can be created, it will be at some point regardless of our opinions on the subject.
So the question is not "Should we create an AGI?" but rather "How do we deal with an AGI once it is created?". It seems likely to me that AGIs will be controllable
just like any piece of software, and any objections raised here suffer from the same AI particularism since humans are perfectly capable of wreaking total havoc
without the aid of an AGI.

In short, I urge every AI alarmist to, *respectfully*, calm their tits.