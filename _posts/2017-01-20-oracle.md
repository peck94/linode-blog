---
title: "An Aaronson Oracle"
date: 2017-01-20
layout: post
---
An *Aaronson oracle* is any computer program that asks a user to type sequences of symbols, such as `f` and `d`, and tries to predict what symbol the user will type next.
It is named after Scott Aaronson, who (as far as I know) first publicly described it. There is a demo of such an oracle online [here](http://people.ischool.berkeley.edu/~nick/aaronson-oracle/).
The motivation behind this oracle is to demonstrate that humans are generally very bad at acting randomly. This challenges the view that randomness is central to human free will, because if humans
suck at acting randomly, then either they don't posses free will or free will has little to do with randomness. The aim of this post is to formally describe an Aaronson oracle, since I have been
unable to find a clean presentation of the theory underlying it. The material presented here will reflect my understanding of the source code of the demo I linked to.

Let $$\Sigma$$ be the alphabet of symbols the user can choose from. Our job is to predict as accurately as we can the symbols the user will type. Let $$S_i$$ denote the $$i$$th symbol the user types,
then this is a discrete random variable taking values from $$\Sigma$$. The goal is to find the probability distribution $$\Pr(S_i)$$ of $$S_i$$ for each $$i$$. To do this, we will make a few assumptions:

1. $$\Pr(S_i)$$ is conditionally independent of all $$S_j$$ for $$j < i - c$$ for some constant $$c$$.
This basically means human working memory is limited: we assume that a user can only remember up to $$c$$ past symbols, so symbols before that have no influence on the current symbol.

2. $$\Pr(S_i \mid S_{i-c}, \dots, S_{i-1})$$ is the same for all $$i$$.
This boils down to the idea that people's behavior is predictable to some degree.

By these assumptions, it is reasonable to model the distribution of $$S_i$$ as a stationary Markov chain of order $$c$$.
The initial state of this chain, $$\Pr(S_1)$$, will represent our total initial ignorance of the user's behavior. Hence, it is the uniform distribution on $$\Sigma$$:
\begin{equation}
    \Pr(S_1 = s) = \frac{1}{|\Sigma|}.
\end{equation}
Then, as the user types more symbols, our algorithm should adjust the probabilities to better predict what the user will type next.
A plausible rule of thumb is the following:

>A user will tend to type the same patterns over and over again.

To model this, we might keep track of the frequencies of all $$c$$-grams (sequences of length $$c$$) the user has typed so far.
To predict what symbol the user will type next, we simply pick the next symbol of the most likely $$c$$-gram the user is currently typing.
This can be done efficiently in both time and space using a [trie](https://en.wikipedia.org/wiki/Trie) where the nodes are augmented to store the absolute frequencies
of the symbols in the current path. The depth of the trie is bounded by the order of the Markov chain. The algorithm conceptually works like this. First, it initializes
a new trie containing just an empty root node. The current node is set to the root node. Then, for every key the user presses, we first predict the symbol that has been typed
by choosing the most likely child of the current node. If the current node has no children, we return a uniform random choice from the alphabet.
The trie is then updated based on which of the following scenarios apply:

1. We are not yet at depth $$c$$ in the trie. In this case, the current path through the trie is extended with a new node labeled with the key the user pressed and an initial frequency of 1.
If the current node already had such a child node, its absolute count is simply incremented. We then set the current node to this child node.

2. We are at depth $$c$$. We then follow the last $$c-1$$ characters the user typed through the trie from the root and set the current node to the node at the end of this path.
Along the way, we create new nodes if necessary and update absolute frequencies.

Below you can find an implementation of this algorithm. Just type in the input box and watch the accuracy of the prediction evolve as you type.
To benchmark the performance of the algorithm, note that on actual uniform random input the accuracy is about 50%. You can verify this experimentally yourself if you want;
just go to [RANDOM.org](https://www.random.org/) or some equivalent site and use true random numbers to guide your decision. So 50% should be your target if you wish to
achieve the same accuracy as a random oracle. However, my own experiments (on myself, of course) have shown that the accuracy usually increases to well beyond 50% for human input.
The order of the chain was set to 5 in this implementation, so if you want to deliberately fool the algorithm, you can always try to construct sequences such that $$\Pr(S_i)$$ is
conditionally dependent on at least 6 previous characters. There is also an entropy score displayed along with the accuracy. This measures the Shannon entropy of the empirical distribution
of $$c$$-grams typed so far. Without going into too much detail about what Shannon entropy is, it suffices to say that you should try to maximize this number. The higher the entropy,
the more closely your sequences match up with uniform random behavior. The entropy will always have a maximum value equal to $$c$$, or 5 in this case.
Note that it is more important to maximize the entropy than it is to minimize the accuracy: if the accuracy is low but the entropy is low as well, then that simply means my particular model
isn't very good at its job, because the empirical distribution is far from uniform. It should, however, be impossible to have an accuracy much higher than 50% when the entropy is close to 5 bits.
The Shannon entropy is nice in that it provides a model-independent assessment of how difficult it would be to predict samples from the distribution.

<input id="oracle" style="width: 100%; font-size: 30px">

<div id="stats"></div>

<div id="results"></div>

<script type="text/javascript">
    // admissible alphabet
    alphabet = "fd";
    // order of the chain
    ORDER = 5;
    // number of correct and wrong guesses
    correct = 0;
    wrong = 0;
    // character buffer
    buffer = [];

    /**
    * Node in the trie.
    * s: symbol of the node
    * d: depth of the node
    **/
    function Node(s, d) {
        // store symbol
        this.label = s;
        // store edges to children
        this.edges = [];
        // store absolute frequency
        this.count = 1;
        // store depth
        this.depth = d;

        // add an edge
        this.addEdge = function(symbol) {
            // if this edge exists already
            if(symbol in this.edges) {
                // increment count
                this.edges[symbol].count++;
            }else{
                // otherwise create new child
                this.edges[symbol] = new Node(symbol, this.depth + 1);
            }

            return this.edges[symbol];
        };

        // predict next symbol
        this.predict = function() {
            // if we have children
            if(!$.isEmptyObject(this.edges)) {
                // return most likely child
                var prediction = undefined;
                for(edge in this.edges) {
                    if(prediction === undefined || prediction.count < this.edges[edge].count) {
                        prediction = this.edges[edge];
                    }
                }

                return prediction.label;
            }else{
                // otherwise return uniform random guess
                return alphabet[Math.floor(Math.random() * alphabet.length)];
            }
        }

        // compute entropy starting from this node
        this.entropy = function(p, t) {
            var q = p * (this.count / t);
            if($.isEmptyObject(this.edges)) {
                return q * Math.log(q) / Math.log(2.0);
            }else{
                var total = 0;
                var r = 0;
                for(edge in this.edges) {
                    total += this.edges[edge].count;
                }
                for(edge in this.edges) {
                    r += this.edges[edge].entropy(q, total);
                }

                return r;
            }
        }

        // follow a path through this trie
        this.find = function(path) {
            if(this.depth >= path.length) {
                return this;
            }

            return this.addEdge(path[this.depth]).find(path);
        }
    }

    // process keys
    function process(key) {
        // add to history
        buffer.push(key);

        // predict this key
        var prediction = current.predict();

        // update trie
        trie.count++;
        current = current.addEdge(key);
        
        // restart from root if necessary
        if(current.depth >= ORDER) {
            // follow last ORDER-1 characters
            buffer.shift();
            console.log(buffer);
            current = trie.find(buffer);
            buffer = [];
        }

        // update stats
        color = (key === prediction) ? "black" : "red";
        $("#results").prepend("<p style='color: " + color + "'>" + (trie.count-1) + ". actual: " + key + "; predicted: " + prediction + "</p>");
        if(key === prediction) {
            correct++;
        }else{
            wrong++;
        }
        acc = Math.round(100 * correct / (correct + wrong));
        $("#stats").html("<p>accuracy: " + acc + "%</p>");
        $("#stats").append("<p>entropy: " + Math.abs(trie.entropy(1, trie.count)) + " / " + ORDER + " bits</p>")
    }

    // test on n random inputs
    function test(n) {
        while(n --> 0) {
            process(Math.random() <= 0.5 ? "f" : "d");
        }
    }

    // handle key presses
    $("#oracle").keypress(function(event) {
        // if key was in alphabet
        var key = String.fromCharCode(event.which);
        if(alphabet.indexOf(key) > -1) {
            process(key);
        }

        $("#oracle").val("");
    })

    // initialize
    trie = new Node('', 0);
    current = trie;
</script>