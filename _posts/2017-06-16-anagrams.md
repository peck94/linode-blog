---
title: Efficient Anagram Detection
date: 2017-06-16
layout: post
tags: complexity
---
So I came across this idea recently:

<figure>
    <center>
        <img src="/assets/img/anagram.png" width="100%">
        <caption>Figure 1. An idea for anagram detection</caption>
    </center>
</figure>

This is a proposal for an efficient algorithm to detect when two words are *anagrams*.
For those unfamiliar with the term, two strings $$s$$ and $$t$$ are anagrams if $$s$$ can be
formed by rearranging the symbols of $$t$$ and vice versa. Basically, the two strings are permutations
of each other. Formally, then, the *anagram decision problem* is the following:

>Given two strings $$s, t$$ over some alphabet $$A$$. Does there exist a permutation $$\phi$$ such that $$\phi(s) = t$$?

Clearly, a necessary (but certainly not sufficient!) condition is that both strings be of equal length.
This is a very simple check which, at worst, runs in time $$\mathcal{O}(\min\{|s|,|t|\})$$; at best, it runs in constant
time since some programming languages keep track of string lengths automatically. Let's assume we always first perform
this check and henceforth only concern ourselves with anagram checking of strings $$s, t$$ where $$|s| = |t| = n$$.
In this post, I will study three possible algorithms for anagram checking and analyze their effectiveness.
We will also assume our alphabet is fixed to the typical Latin alphabet, i.e.

$$
    A = \{ a, b, c, \dots, x, y, z \}.
$$

# The Prime Number Algorithm

The first algorithm that I'll analyze is the one from the picture above.
In pseudocode, it could look like this:

    def prime_anagram(s, t):
        Let f be an injection mapping symbols to primes.
        p1, p2 = 1, 1
        for c1, c2 in zip(s, t):
            p1 *= f(c1)
            p2 *= f(c2)
        
        return p1 == p2

While simple and quite obviously correct from a theoretical point of view, the algorithm is definitely not the most efficient option.
However, the most pressing issue with this algorithm is not its runtime, it's numerical overflow.
In languages that support arbitrary precision integers, this is less of a problem, but implementing this algorithm in (say) plain old C
would quickly become problematic. Suppose we're running a 64-bit system, so a standard integer will occupy 64 bits in memory.
The first 26 primes are

    2      3      5      7      11     13     17     19     23     29 
    31     37     41     43     47     53     59     61     67     71 
    73     79     83     89     97    101

Using these primes would yield an algorithm that would use the least amount of bits on any input.
The number 101 can be represented using at most 7 bits. Take an input $$s$$ consisting solely of symbols corresponding to the
largest prime the injection can provide. The product of a $$b_1$$ and a $$b_2$$ bit integer requires $$b_1+b_2$$ bits,
so a string of length $$n$$ consisting only of one symbol corresponding to the largest prime would take at least $$7b$$ bits.
On a 64 bit architecture, this means our word lengths are limited to about nine characters. Although the average English word
contains about five characters, this is still an undesirable property of the algorithm. What if we want to decide whether two
*sentences* are anagrams? What if we want to run the algorithm on a long name or a long word? The longest English word has 45
characters according to the Oxford English dictionary, well in excess of our puny nine-character limit. That word is
[pneumonoultramicroscopicsilicovolcanoconiosis](https://en.wikipedia.org/wiki/Pneumonoultramicroscopicsilicovolcanoconiosis),
and its product according to the above scheme would be (assuming we map the alphabet to the primes in the natural way, i.e.
a maps to 2, b maps to 3, etc.)

    208741561674716660518143595093314485576064635609153569802552562500

This integer takes 217 bits to represent, so arbitrary precision integers would be required to compute its product.
Although multiplication of large integers can be done efficiently, there is no known algorithm which takes only linear
time in the number of bits to be multiplied. All efficient algorithms for multiplication on arbitrary precision integers
take time $$\mathcal{O}(n^c)$$ where $$c > 1$$, making this a super-linear algorithm.

**Choice of mapping**.
One might think that changing the mapping could improve performance. For example, what if we were to map the different symbols
in the Latin alphabet by their frequency in the English language? That is, we map more commonly used symbols to smaller primes.
It should be clear that such a modification would not improve the worst-case running time of the algorithm, but it might improve
the average case.

**Using logarithms**.
One could also suggest that taking logarithms and computing sums instead of products would improve the numerical precision.
However, for this to work, we would have to utilize floating point precision, which opens its own can of worms regarding rounding
and truncation errors.

To summarize, the algorithm described above suffers from the following problems:

1. It runs in super-linear time.
2. It suffers from numerical precision issues.

It would be nice to have an algorithm for solving the anagram problem that runs in linear time and does not have numerical issues.

# Detecting anagrams by sorting

An arguably much more simple idea for detecting anagrams is the following algorithm:

    def anagram_sorted(s, t):
        Sort the characters of s and t lexicographically.
        return s == t

Again, this algorithm is obviously correct. Its runtime is also clearly $$\mathcal{O}(n \log n)$$ if an efficient sorting algorithm is used.
By comparison, the most efficient multiplication algorithm at the time of this writing appears to be [FÃ¼rer's algorithm](https://en.wikipedia.org/wiki/F%C3%BCrer%27s_algorithm),
which runs in time $$\mathcal{O}(2^{\mathcal{O}(\log^\star n)} n \log n)$$ which is slightly worse.
Hence, this algorithm is both asymptotically faster than the prime number variant and does not suffer from any numerical issues.
But we can do better.

# Detecting anagrams in linear time

We can detect anagrams in linear time as follows:

    def anagram_linear(s, t):
        d = {
            'a': 0
            ...
            'z': 0
        }
        for c1, c2 in zip(s, t):
            d[c1] += 1
            d[c2] -= 1
        
        for c in d:
            if d[c] != 0:
                return False
        return True

What this algorithm does is the following:

1. Allocate a dictionary where the characters in the alphabet are the keys. Initially, all values are set to zero.
2. For each character in $$s$$, increment the corresponding integer in the dictionary.
3. For each character in $$t$$, decrement the corresponding integer in the dictionary.
4. Return true if all values in the dictionary are zero and false otherwise.

Correctness is again easy to see. The running time of this algorithm is also clearly $$\mathcal{O}(n)$$ assuming the underlying
hash function of the dictionary maps each symbol to its own bucket. Since we only use the Latin alphabet, this is a safe assumption.
(The hash would have to be exceedingly shitty for it not to satisfy this property.)
