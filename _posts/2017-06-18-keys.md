---
title: Sizes of Cryptographic Keys
date: 2017-06-18
layout: post
tags: infosec
---
When using any form of public or private key cryptography, the security of the algorithms almost always boils down to the size of the keys you pick [[^1]].
You do not usually get to pick the key manually, however: often, a key generation algorithm is used which only asks this single question:

>How many bits should the key have?

In general, a longer key is always more secure, so it makes sense to pick the largest key size which is still feasible for you to work with.
However, if a smaller key can be used without compromising security, this is of course very desirable. As such, one often wants to base the
key size on the question of how long one wants to keep the data protected by the key secure, and not make the key any longer than strictly
necessary.

So how do we determine the optimal key size? The answer relies on the nature of the cryptosystem one uses.
In the private key setting, matters are usually rather straightforward. If the symmetric cipher is any good, an $$n$$ bit key should yield
$$\mathcal{O}(2^n)$$ security, meaning an attacker cannot do much better than an exhaustive search on all possible keys.
In the public key setting, however, matters become a little more complicated. Public key cryptosystems are always based on problems for which
no efficient (i.e. polynomial-time) algorithm is known. Typical examples of such problems include
[factoring large integers](https://en.wikipedia.org/wiki/Integer_factorization) and solving the [discrete logarithm problem (DLP)](https://en.wikipedia.org/wiki/Discrete_logarithm).

In general, cracking a public key cryptosystem boils down to solving such hard problems.
For instance, one should not be able to crack RSA without efficiently factoring integers or solving the DLP.
How hard these problems are for any particular instance of RSA depends on the key size, so we want to set the key size such that
these problems cannot be solved within the time span we need the data to be secure.

# RSA

As a concrete example, suppose we have an $$n$$ bit [RSA](https://en.wikipedia.org/wiki/RSA_(cryptosystem)) modulus $$N$$.
Anyone who can factor this modulus can break our system, so we need to find out what the most efficient algorithm is for factoring large integers
and make sure our $$N$$ is so large that even the most efficient known factoring algorithm cannot factor it in reasonable time.
At the time of this writing, the fastest known algorithm for factoring is the [General Number Field Sieve (GNFS)](https://en.wikipedia.org/wiki/General_number_field_sieve).
This algorithm runs in time proportional to

$$
    \mathrm{GNFS}(N) = \exp\left( \left(\left(\frac{64}{9}\right)^{1/3} + o(1)\right)(\ln N)^{1/3}(\ln\ln N)^{2/3} \right).
$$

Here, $$N$$ is the integer to be factorized. In this case, it's our RSA modulus.
The current record for integer factorization is [RSA-768](https://en.wikipedia.org/wiki/RSA-768), which is a 768 bit integer factored in 2009.
The GNFS can factor such an integer in time proportional to approximately $$\exp(53) \approx 2^{76}$$.
Hence, thanks to the GNFS, a 768 bit RSA modulus has an effective key length of 76 bits, which was feasible to crack in 2009.
Taking into account [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law), processing speeds are about 16 times higher in 2017 than they
were in 2009. If we want our RSA modulus in 2017 to still be hard to factor, it must have an effective key size exceeding 92 bits.
We thus have to solve the following equation for $$N$$:

$$
    \mathrm{GNFS}(N) > 2^{92} \approx \exp(64).
$$

We find $$N \approx 2^{1182}$$, so our modulus should have at least 1182 bits. Rounding up to the most comfortable power of 2, we simply take
2048 bits. We then have

$$
    \mathrm{GNFS}(N) \approx \exp(81) \approx 2^{116}.
$$

Thus, a 2048 bit RSA modulus has an effective key length of 116 bits. This gives us a comfortable margin of 24 bits over the minimum of 92.

# DSA

As another example, consider the [Digital Signature Algorithm (DSA)](https://en.wikipedia.org/wiki/Digital_Signature_Algorithm).
This algorithm can be used to generate secure digital signatures provided we choose the following parameters carefully:

1. A prime $$p$$ which determines a group $$\mathbb{Z}_p^\star$$.
2. A prime $$q$$ such that $$q \mid (p-1)$$, yielding a (cyclic) prime-order subgroup of $$\mathbb{Z}_p^\star$$.
3. An $$\ell$$-bit hash function $$H$$.

Each of these parameters is vulnerable to some attack:

1. The group $$\mathbb{Z}_p^\star$$ is vulnerable to factoring by the GNFS.
2. The prime-order subgroup of $$\mathbb{Z}_p^\star$$ can be factored using [Pollard's rho algorithm](https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm).
3. The hash function is vulnerable to a [Birthday attack](/2017/06/07/birthday.html).

We already know the runtime of the GNFS as a function of $$p$$. Pollard's rho method runs in time proportional to $$\sqrt{q}$$,
and a Birthday attack requires about $$2^{\ell/2}$$ hash evaluations. We want all of these algorithms to require approximately the same
amount of time, so that all of the parameters are equally secure. Indeed, the entire signature scheme is only as strong as its weakest
link, so we must ensure all links are about the same strength. Thus, we want to find primes $$p,q$$ and an $$\ell$$-bit hash so that

$$
    \mathrm{GNFS}(p) \approx \sqrt{q} \approx 2^{\ell/2}.
$$

Let's take a 2048 bit prime $$p$$, which we know to have an effective security of about $$2^{116}$$.
We then pick $$q$$ to satisfy

$$\begin{aligned}
    \sqrt{q} &\approx 2^{116}.
\end{aligned}$$

Hence $$q \approx 2^{232}$$, so $$q$$ should be a 232 bit prime. The hash function should then satisfy

$$
    2^{\ell/2} \approx 2^{116},
$$

meaning $$\ell \approx 232$$. To summarize:

1. We pick a 2048 bit prime $$p$$.
2. We pick a 232 bit prime $$q$$ such that $$q \mid (p-1)$$.
3. The hash function should have an output size of about 232 bits, such as [SHA-256](https://en.wikipedia.org/wiki/SHA-2).

# Conclusion

When one wishes to determine the optimal parameters for any cryptosystem, the following general strategy may be followed:

1. Determine the hardness assumptions of the cryptosystem. What problems does the cryptosystem rely on to be hard?
2. Find out what the state-of-the-art algorithms are for solving those problems.
3. Let the parameters for the cryptosystem be such that all problems are approximately equally hard given the state of the art
and unfeasible for the foreseable future unless significantly more efficient algorithms are found.

The first two steps are easy since any good specification of a cryptosystem will clearly state the hardness assumptions and may
even provide an overview of the state-of-the-art algorithms for solving those problems at the time of writing.
The third step involves some math and requires one to know where the current threshold of feasibility lies.
Competitions such as the [RSA Challenges](https://en.wikipedia.org/wiki/RSA_Factoring_Challenge) serve to quantify this threshold.
If specific details for the current year are unavailable (such as in our RSA example), one simply extrapolates based on Moore's Law.

That being said, in practice it is much easier (and safer!) to simply rely on recommendations made by dedicated organisations such as [NIST](https://www.nist.gov/).

# Footnotes

[^1]: If there are other factors determining the strength of the cryptosystem, that's usually an indication that it's not very strong and should not be used.
