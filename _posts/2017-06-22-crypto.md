---
title: Overview of Asymmetric Cryptography
layout: post
date: 2017-06-22
tags: infosec
---
The asymmetric cryptosystems used in practice fall roughly into two categories:

1. Cryptosystems based on hardness of factoring.
2. Cryptosystems based on hardness of the discrete logarithm problem.

In this post, I will attempt to provide an overview of how these systems work at a high level.
I will cover how encryption and decryption is done in each of these systems as well as how to
generate and verify their digital signatures.

# Factoring

The *factoring problem* is the problem of determining non-trivial factors of a given integer $$N$$.
The *factoring assumption* is the assumption that factoring is hard relative to some method of generating composite integers $$N$$.
Formally, we define the *factoring experiment* as follows. Given an algorithm $$\mathcal{G}$$ for generating $$n$$-bit primes and
an adversary $$\mathcal{A}$$:

1. Run $$\mathcal{G}(n)$$ to obtain two $$n$$-bit primes $$p$$ and $$q$$.
2. Compute $$N = pq$$.
3. Give $$N$$ to the adversary $$\mathcal{A}$$. The adversary outputs integers $$p^\prime, q^\prime$$.
4. The experiment succeeds if $$p^\prime, q^\prime$$ are non-trivial factors of $$N$$, i.e. $$p^\prime, q^\prime > 1$$ and $$N = p^\prime q^\prime$$.

The factoring assumption states that there exists an efficient algorithm $$\mathcal{G}$$ such that for any efficient adversary $$\mathcal{A}$$,
the factoring experiment relative to $$\mathcal{G}$$ and $$\mathcal{A}$$ can only succeed with negligible probability.

To be more formal, an *efficient algorithm* is an algorithm that runs in probabilistic polynomial time in $$n$$, i.e.
the algorithm is allowed to be randomized but it must always run in polynomial time.
A function $$f$$ is said to be *negligible* if it grows asymptotically more slowly than any inverse polynomial, i.e.
for any polynomial $$p$$ it holds that

$$
    \exists N: \forall n > N: f(n) < \frac{1}{p(n)}.
$$

For example, $$f(n) = 2^{-n}$$ is a negligible function.
Hence, the factoring assumption states that there exists an algorithm $$\mathcal{G}$$ that runs in probabilistic polynomial time in $$n$$ such that for
any probabilistic polynomial time (in $$n$$) adversary $$\mathcal{A}$$ the probability that $$\mathcal{A}$$ is able to factor the integers generated by
$$\mathcal{G}$$ is a negligible function of $$n$$.

Put briefly, this simply means factoring is assumed to be a hard problem.

## RSA

The best known cryptosystem based on hardness of factoring is *RSA*, named after its authors Rivest, Shamir and Adleman.
The security of RSA actually does not depend solely on the factoring problem; it depends on the *RSA assumption*, which states
that the *RSA experiment* is hard to complete successfully. The RSA experiment relative to two algorithms $$\mathcal{G}$$ and
$$\mathcal{A}$$ consists of the following steps:

1. Run $$\mathcal{G}(n)$$ to obtain two $$n$$-bit primes $$p$$ and $$q$$.
2. Compute $$N = pq$$ and $$\varphi(N) = (p-1)(q-1)$$.
3. Choose $$e > 1$$ such that $$\gcd(e, \varphi(N)) = 1$$ and $$d$$ such that $$ed \equiv 1 \mod \varphi(N)$$.
4. Choose a uniform random element $$y \in \mathbb{Z}_N^\star$$.
5. The adversary $$\mathcal{A}$$ is given $$N$$, $$e$$ and $$y$$. The adversary outputs an element $$x \in \mathbb{Z}_N^\star$$.
6. The experiment succeeds if $$x \equiv y^d \mod N$$.

The RSA assumption then states that there exists an efficient algorithm $$\mathcal{G}$$ such that for all efficient adversaries $$\mathcal{A}$$,
the probability of the adversary successfully completing the RSA experiment is negligible in $$n$$.
Note that hardness of the RSA experiment does indeed rely on hardness of factoring in that if factoring is easy, the RSA problem is easy as well.
However, it is not known at this time whether the converse is true as well, i.e. whether factoring is equivalent to the RSA problem.
It may be that RSA can be efficiently broken without resorting to factoring the modulus $$N$$.

**Key generation**. To generate a pair of RSA keys $$(pk, sk)$$, we perform the following operations:

1. Generate two $$n$$-bit primes $$p$$ and $$q$$. Compute $$N = pq$$, $$\varphi(N) = (p-1)(q-1)$$.
2. Compute $$e > 1$$ such that $$\gcd(e,\varphi(N)) = 1$$ and $$d$$ such that $$ed \equiv 1 \mod \varphi(N)$$.
3. The public key is $$(N,e)$$; the private key is $$(p,q,d)$$.

**Encryption**. To encrypt a plaintext message $$m \in \mathbb{Z}_N^\star$$, we compute the ciphertext $$c$$ as

$$
    c \equiv m^e \mod N.
$$

**Decryption**. To decrypt a ciphertext $$c \in \mathbb{Z}_N^\star$$, compute

$$
    m \equiv c^d \mod N.
$$

**Signature generation**. An RSA signature $$\sigma$$ on a message $$m$$ is computed as

$$
    \sigma \equiv m^d \mod N.
$$

**Signature verification**. To verify a signature $$\sigma$$ on a message $$m$$, check that

$$
    m \equiv \sigma^e \mod N.
$$

**Correctness**. Correctness of both encryption/decryption and signing/verifying is easily checked by noting that, by Euler's theorem, we have

$$
    c^d \equiv (m^e)^d \equiv m^{ed} \equiv m^{k\varphi(N) + 1} \equiv m \mod N.
$$

# Discrete logarithm

The *discrete logarithm problem* is the problem of computing $$\log_g(g^x)$$ in a cyclic group $$(\mathbb{G}, q, g)$$.
That is, we are given a group with elements $$\mathbb{G}$$, order $$q$$ and generator $$g$$.
We are then asked to compute $$x$$ given a group element $$g^x$$.
This problem is assumed to be hard relative to some algorithm for generating cyclic groups, in particular prime-order
subgroups of $$\mathbb{Z}_p^\star$$ as well as elliptic curve groups.

The *discrete logarithm experiment* formalizes this assumption as follows:

1. Run $$\mathcal{G}(n)$$ to generate a cyclic group $$(\mathbb{G}, q, g)$$ with $$n$$-bit order $$q$$.
2. Choose a uniform random element $$h \in \mathbb{G}$$.
3. Give $$h$$ to the adversary $$\mathcal{A}$$. The adversary returns $$x \in \mathbb{Z}_q$$.
4. The experiment succeeds if $$h = g^x$$.

The discrete logarithm assumption now states that there exists an efficient algorithm $$\mathcal{G}$$ such that all efficient
adversaries $$\mathcal{A}$$ have only a negligible probability (as a function of $$n$$) of succeeding in this experiment.

Two related assumptions are the *Computational Diffie-Hellman* (CDH) and *Decisional Diffie-Hellman* (DDH) assumptions.
The CDH assumption states that the following function is hard to compute for certain groups:

$$
    \mathrm{DH}_g(h_1, h_2) = g^{(\log_gh_1)(\log_gh_2)}.
$$

That is, if we are given a cyclic group $$(\mathbb{G}, q, g)$$ and two elements $$g^x, g^y \in \mathbb{G}$$, it is hard to compute $$g^{xy}$$.
Note that this assumption depends on hardness of the discrete log. Indeed, if the discrete log is easy to compute, then $$\mathrm{DH}_g$$ is
clearly easy to compute as well. It is not known at the time of this writing, however, whether the converse is true; it may be that there exists
an efficient algorithm for computing $$\mathrm{DH}_g$$ in any group yet no efficient algorithm for the discrete log.

The DDH assumption states that given a group $$(\mathbb{G}, q, g)$$ and two elements $$g^x, g^y \in \mathbb{G}$$, no efficient algorithm exists
that (with non-negligible probability) can distinguish $$g^{xy}$$ from a uniform random group element.
This is clearly the strongest of all the discrete log related assumptions, since hardness of the DDH problem depends on hardness of the CDH
problem, which in turn depends on the discrete log. However, this is in fact the most important assumption since security of virtually all
discrete log based cryptosystems depends on the DDH assumption.

## ElGamal

The most prominent asymmetric cryptosystem based on the discrete logarithm problem is *ElGamal*, named after its author
Taher ElGamal. The description given here, however, is a generalised version of this algorithm which works with any cyclic group,
whereas the original formulation of ElGamal was specialized for prime-order subgroups of $$\mathbb{Z}_p^\star$$.

The digital signature scheme presented here is also called the *Digital Signature Algorithm* (DSA).
Again, the description of this algorithm in this post is generalised to arbitrary cyclic groups and so also covers the case where
the group is an elliptic curve. The specialized version for elliptic curves is called the *Elliptic Curve Digital Signature Algorithm* (ECDSA).

**Key generation**. The public and private keys are determined as follows:

1. Generate a cyclic group $$(\mathbb{G}, q, g)$$ with an $$n$$-bit order $$q$$.
2. Choose a uniform random element $$x \in \mathbb{Z}_q$$ and compute $$y = g^x$$.
3. The public key is $$(\mathbb{G}, q, g, y)$$; the private key is $$(\mathbb{G}, q, g, x)$$.

**Encryption**. To encrypt a plaintext message $$m \in \mathbb{G}$$:

1. Choose a uniform random element $$k \in \mathbb{Z}_q$$ and compute $$z = g^k$$.
2. Compute $$s = my^k$$.
3. The ciphertext is the pair $$(z, s)$$.

**Decryption**. To decrypt a ciphertext $$(z, s)$$, we compute

$$
    m = s(z^x)^{-1}.
$$

**Signature generation**. To sign a message $$m$$:

1. Generate a uniform random element $$k \in \mathbb{Z}_q^\star$$ and compute $$r = g^k$$.
2. Compute $$s = k^{-1}(H(m) + xr) \mod q$$ where $$H$$ is a cryptographic hash function.
3. The signature is the pair $$(r, s)$$.

**Signature verification**. To verify a signature $$(r,s)$$ on a message $$m$$, check that

$$
    r = g^{H(m)s^{-1}}y^{rs^{-1}}.
$$

**Correctness**. To see that encryption and decryption are correct, we check

$$
    s(z^x)^{-1} = my^k\left( g^{xk} \right)^{-1} = mg^{xk}g^{-xk} = m.
$$

To verify the digital signatures:

$$\begin{aligned}
    g^{H(m)s^{-1}}y^{rs^{-1}} &= g^{H(m)s^{-1}}g^{xrs^{-1}}\\
        &= g^{H(m)s^{-1} + xrs^{-1}}\\
        &= g^{(H(m) + xr)s^{-1}}\\
        &= g^{(H(m) + xr)(H(m) + xr)^{-1}k}\\
        &= g^k\\
        &= r.
\end{aligned}$$

## Elliptic curves

Another cryptographic scheme worth mentioning here is *Elliptic Curve Cryptography* (ECC).
As the name implies, this scheme is based on *elliptic curves*. Specifically, elliptic curves over finite fields.
Let $$\mathbb{F}$$ be any finite field, then we can define the elliptic curve $$E(\mathbb{F})$$ over this field as the following set:

$$
    E(\mathbb{F}) = \{ (x,y) \in \mathbb{F}^2 \mid y^2 = x^3 + ax + b \} \cup \{ \mathcal{O} \}.
$$

Here, $$a$$ and $$b$$ are elements of $$\mathbb{F}$$ and $$\mathcal{O}$$ is a special element called the *point at infinity*.
The set $$E(\mathbb{F})$$ forms a group under the operation of addition, where addition of points on an elliptic curve is defined
in a rather peculiar way. Figure 1 illustrates geometrically how one computes the sum of two points on an elliptic curve.

<figure>
    <center>
        <img src="/assets/img/ec.png">
        <br>
        <caption>Figure 1. Addition of points on an elliptic curve</caption>
    </center>
</figure>

Specifically, to compute the sum $$P + Q$$ of two points $$P, Q \in E(\mathbb{F})$$, one first determines the straight line
passing through $$P$$ and $$Q$$. This line may intersect the elliptic curve at a third point or it may never intersect the curve again.
In the former case, we let that point be $$-R$$. The sum $$P+Q$$ is then obtained by reflecting $$-R$$ about the x-axis, yielding $$R$$.
If the line never intersects the curve again, the sum of the points is $$\mathcal{O}$$.
The point at infinity $$\mathcal{O}$$ serves as the neutral element for point addition.

Now, given the definition of addition of points on an elliptic curve, we can define scalar multiplication of a point $$P$$ by a scalar $$a$$
in the obvious way:

$$
    aP = \underset{a\,\mathrm{ times}}{\underbrace{P + \dots + P}}.
$$

It is important to note that elliptic curves defined over finite fields will have a finite *order* $$q$$ such that for any point $$P$$ on
the curve, $$qP = \mathcal{O}$$.

We are now ready to describe how ECC works. The security of this scheme relies on the *Elliptic Curve Discrete Logarithm Problem* (ECDLP), which
is just a specialized version of the ordinary DLP for elliptic curves. Specifically, it states that given two points $$P$$ and $$aP$$ on the curve
(where $$a$$ is a scalar), it is hard for an efficient algorithm to determine $$a$$ with non-negligible probability.

**Key generation**. To generate a public/private pair of elliptic curve keys:

1. Choose an elliptic curve $$(E(\mathbb{F}), q, P)$$ over some finite field $$\mathbb{F}$$ with $$n$$-bit order $$q$$ and a point $$P$$ on the curve.
2. Choose a uniform random element $$a \in \mathbb{Z}_q$$ and compute $$Q = aP$$.
3. The public key is $$(E(\mathbb{F}), q, P, Q)$$; the private key is $$(E(\mathbb{F}), q, P, a)$$.

**Encryption**. To encrypt a plaintext $$M \in E(\mathbb{F})$$:

1. Choose a uniform random element $$b \in \mathbb{Z}_q$$.
2. Compute $$c_1 = bP$$ and $$c_2 = M + bQ$$.
3. The ciphertext is $$(c_1, c_2)$$.

**Decryption**. To decrypt a ciphertext $$(c_1, c_2)$$, compute

$$
    M = c_2 - ac_1.
$$

**Signature generation**. To sign a message $$M$$:

1. Choose uniform random $$k \in \mathbb{Z}_q^\star$$.
2. Compute $$R = kP$$. Let $$r$$ be the $$x$$-coordinate of $$R$$, and compute $$s = k^{-1}(H(M) + ar)$$ where $$H$$ is a cryptographic hash function.
3. The signature is $$(r, s)$$.

**Signature verification**. To verify a signature $$(r,s)$$ on a message $$M$$:

1. Compute $$u_1 = H(m)s^{-1}$$ and $$u_2 = rs^{-1}$$.
2. Calculate $$R = u_1P + u_2Q$$.
3. Check that the $$x$$-coordinate of $$R$$ is equal to $$r$$.

**Correctness**. The correctness of ECC follows from the correctness of generalised ElGamal, since ECC is just a special case of this algorithm.
