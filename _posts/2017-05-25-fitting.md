---
title: Curve Fitting
date: 2017-05-25
layout: post
---
Many problems in science boil down to finding a function $$f$$ which models certain phenomena.
In essence, we have a set $$\mathcal{D}$$ of observations and we want to find the simplest possible
function which fits these observations. This process of finding a curve which maps observations to
appropriate outputs is also called "regression".

In its simplest form, we have a data set of observations $$\mathcal{D} = \{ (x_i, y_i) \mid i = 1, \dots, N \}$$
where $$x_i, y_i \in \mathbb{R}$$ for all $$i$$. This is the simplest case because it makes a few assumptions
which do not hold in general:

* the data is one-dimensional, i.e. each $$x_i$$ is just a real number;
* the data is noiseless.

In reality, we usually want to find functions $$f: \mathbb{R}^n \to \mathbb{R}$$ which map many different variables to a single output [[^1]].
Moreover, in practice, the readings we take will always be noisy. We'll start with the above assumptions, though, in order to ease into the theory.

The goal now is to find a function $$f$$ such that $$f(x_i) = y_i$$ for all $$i$$.
The crucial insight here is the fact that without any assumptions on the underlying process that generated the data set, we are stuck.
Consider, for example, the well-known problem of determining the next number in a sequence.
Suppose I give you this partial list:

    1, 1, 2, 3, 5

I now ask of you to find the next number in the sequence I have in mind.
You may believe this to be the [Fibonacci sequence](https://en.wikipedia.org/wiki/Fibonacci_number) which starts with $$y_0 = y_1 = 1$$
and then sets $$y_{k+2} = y_{k+1} + y_k$$. However, the sequence I had in mind was actually

    1, 1, 2, 3, 5, 5, 3, 2, 1, 1

Could you have known this? No. Nobody could have reasonably guessed this in the absence of any additional information.
These additional assumptions we impose on the nature of the process that generated our data set is called *inductive bias*,
since it biases our inductive procedure to certain classes of functions. Inductive bias is useful in that it allows an otherwise
intractable problem to become tractable. However, if our inductive bias is incorrect, we may never succeed in finding a function
which correctly fits all the data, so it's clearly a double-edged sword.

In our present case, where we want to estimate a function $$f: \mathbb{R} \to \mathbb{R}$$ which fits $$\mathcal{D}$$, we may assume
the underlying process uses a linear combination of *basis functions* $$\phi_0, \dots, \phi_k$$ so that

$$
    f(x) = \sum_{i=0}^k a_i\phi_i(x).
$$

If we let $$\phi_i(x) = x^i$$, then our inductive bias is that the data set can be modeled by a polynomial of degree at most $$k$$.
These basis functions may be much more general than just polynomials, of course.
Using this framework, we find we want to satisfy the following equations:

$$\begin{aligned}
    \sum_{i=0}^k a_i\phi_i(x_1) &= y_1\\
    &\vdots\\
    \sum_{i=0}^k a_i\phi_i(x_n) &= y_n
\end{aligned}$$

This is equivalent to solving the following system of equations:

$$
    Wa = y.
$$

Here, the components of the matrix $$W$$ are given by

$$
    w_{ij} = \phi_j(x_i).
$$

Hence, the coefficient vector $$a$$ can be found using techniques of linear algebra, even though the basis functions may be highly non-linear.

<figure>
    <center>
        <img src="/assets/img/fit.png">
        <figcaption>Figure 1. An example curve to be fit</figcaption>
    </center>
</figure>

As an example, consider the curve in Figure 1.
Suppose we sample this curve at discrete points so that

$$
    \mathcal{D} = \{ (-1, -24.26182), (0, -10), (1, 15.46133) \}.
$$

Assume also that our inductive bias leads us to believe the function is of the form

$$
    f(x) = a_1\exp(-x) + a_2\cos(x) + a_3\sin(x).
$$

We then find

$$
    \left[\begin{matrix}
        \exp(1) & \cos(-1) & \sin(-1)\\
        \exp(0) & \cos(0) & \sin(0)\\
        \exp(-1) & \cos(1) & \sin(1)
    \end{matrix}\right]
    \cdot
    \left[\begin{matrix}
        a_1\\
        a_2\\
        a_3
    \end{matrix}\right]
    =
    \left[\begin{matrix}
        -24.26182\\
        -10\\
        15.46133
    \end{matrix}\right].
$$

Since this system is square, it can be solved exactly to yield

$$
    a = \left[\begin{matrix}
        1\\
        -11\\
        25
    \end{matrix}\right].
$$

In practice, however, our system will rarely be square. It is much more common to have rectangular systems
where we have many more data points (i.e. rows) than variables to be inferred (i.e. columns). In such a case,
we use an approximation such as linear least squares to find the closest fit to the data.
This approach will also work if the data contains unstructured noise.
If we have reason to believe structured noise was added to the data, though, we may be able to apply a specialized
denoising procedure to preprocess the data before fitting the curve. This may lead to much better fits.

## Footnotes

[^1]: In case we want to find functions which map $$n > 1$$ variables to $$m > 1$$ variables, this is essentially the problem of finding $$m$$ functions which each map $$n$$ variables to a single output. So this is a simplifying assumption we *can* make without loss of generality.