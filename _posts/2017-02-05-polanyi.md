---
title: "Polanyi's Paradox"
layout: post
date: 2017-02-5
---
The philosopher Michael Polanyi stated in 1966:

>We can know more than we can tell...
The skill of a driver cannot be replaced by a thorough schooling in the theory of the motorcar;
the knowledge I have of my own body differs altogether from the knowledge of its physiology.

Intuitively, he seems to be absolutely correct. Who among us can, for example, write down an algorithm which explains step by step
how to recognize a face in an image, or how to drive a car, or how to make sweet sweet love? Some smartass may answer that some of
these problems, such as facial recognition, can be solved by machine learning algorithms. This is true, but it's emphatically not
my point. My point is that, at the time of this writing, either algorithms for solving these tasks are unknown or they are not
*interpretable*. That is, in cases such as facial recognition, we do have models such as Convolutional Neural Networks (CNNs) that provide
algorithms for solving certain problems. But these algorithms are not interpretable in the sense that it is currently unclear why they
work; it is unknown what the precise relationship is between the algorithm that is run on the computer and the correctness of the output.
To define the notion of interpretability more precisely for the mathematically inclined among you who object to vague definitions, let me
put it this way:

>An algorithm is called *interpretable* if one can write down a formal proof of correctness of this algorithm in some formal system (e.g. ZFC or PA).

This definition certainly captures the intuition that if you cannot produce a correctness proof of an algorithm, you do not fully understand how or
why it works and hence it cannot be said to be interpretable. In that sense, CNNs are not interpretable since, to my knowledge, researchers are yet
to provide a formal proof of correctness that a given CNN will recognize desired objects in images which contain them with high confidence.
Of course, some genius may be able to produce a proof which that genius alone understands but nobody else does; this can be seen as a shortcoming of
the definition, but then the algorithm is interpretable to at least one person, so there's that (assuming the proof is correct, naturally).

But I digress. The reason why I find Polanyi's paradox worth writing about is not only because, like Occam's Razor, it is a nice, intuitively correct
observation; I find it interesting because, in my view, it is sometimes misused to make poorly founded claims about the extent to which we will be able
to automate human tasks. A common argument goes essentially like this:

>Polanyi's paradox entails that we cannot provide explicit descriptions for solving certain problems.
Yet we need such explicit descriptions if we are to ever create algorithms for automating these tasks.
Hence we will not be able to automate everything humans can do.

To the casual eye, this seems like a sound argument. After all, both premises seem to be absolutely correct and the conclusion follows logically from
the premises. While I do believe this argument to be *sound* in the sense that the conclusion does indeed follow logically from the premises, I object
to at least one of the premises. I will grant that Polanyi's paradox seems very plausible and I will assume that much of the argument is true: that
there exists knowledge which we possess but cannot articulate explicitly. The second premise, however, I believe to be false, for two reasons.
First, the recent advances in deep learning offer an empirical counter-example to the claim that we need to be able to explicitly describe how to
solve certain problems in order to code an algorithm for doing so automatically. Most of the machine learning techniques used nowadays are pretty
much black boxes: they are statistical models which we train with massive amounts of data on big distributed computing clusters, but nobody knows
exactly why they work; they are not interpretable. Yet they are highly successful, sometimes achieving even superhuman performance on certain tasks
previously thought almost unsolvable by computers.

But there is a second, more profound and theoretical reason to think the second premise is false.
It requires a considerable amount of exposition, however, which I will try to present in an understandable and succinct fashion.
To appreciate why the second premise may not be valid, we must take a quick introduction to the field of computational learning theory.
This field is dedicated to studying the theoretical properties of learning algorithms, and it has produced some fascinating results.
Specifically, in the so-called *PAC-model* (Probably Approximately Correct model) of learning, we formalize supervised learning as follows.
Say we have some dataset $$D = \{ (x_i, y_i) \mid i = 1, \dots, n \}$$ of samples where each $$x_i$$ is an instance of our problem and
each $$y_i$$ is the correct solution for that instance. For example, each instance may encode an assignment of Boolean values to a set $$X_1, \dots, X_k$$ of
variables and each $$y_i$$ may be a bit indicating whether some Boolean formula is true or false given those assignments. The object of the
learning problem may then be to seek out which Boolean formula best explains the observed data, i.e. the object is to find a Boolean formula
$$\phi$$ in the variables $$X_1, \dots, X_k$$ such that for each $$i$$, $$\phi(x_i) = y_i$$.
Without going too much into the technical details, a learning problem such as the example above is said to be *PAC-learnable* if there exists a
polynomial-time algorithm which outputs a hypothesis explaining the observed data with an average error of at most $$\varepsilon$$ with probability
at least $$1 - \delta$$. Here, $$\varepsilon$$ and $$\delta$$ are parameters of the algorithm which can be any real number between 0 and 1 exclusive,
and the algorithm has to be polynomial-time for any setting of those values.

There are many other subtleties which I have deliberately omitted since they would distract from the central point I want to make here.
Specifically, using this rather informal working definition of PAC-learning, I want to try to convince the reader that the second premise of the
original argument using Polanyi's paradox is unfounded. There is a well-known result in computational learning theory which lends credibility to
this claim, namely the fact that the same problem may suddenly go from being PAC-learnable to being intractable depending on what output format one demands.
For example, in the case of learning Boolean formulae, we may demand the output hypothesis to be a formula in Conjunctive Normal Form (CNF) or
Disjunctive Normal Form (DNF). Logically, there is no difference between these two formats since any Boolean formula can be written in CNF as well as DNF;
**but the problem of learning Boolean formulae in CNF is PAC-learnable, while the problem of learning Boolean formulae in DNF is not!**
The way I interpret this result, this is in fact a formal version of a variant Polanyi's paradox. What this means is that the way we are expected to present
a solution to a problem may make the difference between learning being feasible and learning being infeasible. We can efficiently learn a formula which is
(probably approximately) equivalent to an arbitrary DNF formula, so long as we are allowed to present this formula in CNF. If we must present it as a
DNF formula, the problem becomes intractable.

Transposing the above result to the context of automating human tasks, we can interpret it as follows.
It may be intractable to provide a solution to a learning problem such as facial recognition in the form of an interpretable algorithm;
but if we are not constrained to produce interpretable results, then the problem may very well be tractable. The fact that this can be done is demonstrated
by several deep learning techniques such as CNNs: they are not interpretable, but they clearly solve the problems they are supposed to.

To be sure, this is all speculative. The only reason I say deep learning techniques are not interpretable is because, at present, no one has yet provided an
interpretation. This could change at any moment, and it would invalidate my argument entirely. I also do not know of any hard evidence that learning the human
tasks referenced by the original argument can become tractable if we are not restricted to interpretable algorithms. Proving this would require constructing
such algorithms, which has not yet been done. However, though not hard evidence in the sense of a mathematical proof, the claims I have presented here can
serve as a motivation for *believing* why the argument from Polanyi's paradox is false. It is no actual proof, but it has certain advantages that the argument
from Polanyi's paradox does not have. For one, there is empirical evidence which so far seems to support my argument; the same cannot be said for the objection,
which relies on a possibly unwarranted extrapolation of previous experience. Perhaps more importantly, though, my argument can be proven by constructive means:
it can be proven by constructing (uninterpretable) algorithms which solve tasks thought accessible only to human intellect. The argument from Polanyi's paradox
is a negative result, and can only be proven by an impossibility theorem. These are notoriously hard in general, and I do not believe any such proof will ever
be forthcoming. That being said, this is all just food for thought; this article is not meant to be taken as some actual proof of anything.